[
  {
    "control_id": "2.3",
    "control": "Address Unauthorized Software",
    "IG1": "o",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889472,
    "recommendation_id": 4683055,
    "view_level": "5.10.4",
    "title": "Ensure use of Binary Authorization",
    "pivot_control_id": 371,
    "pivot_recommendation_id": 4683055,
    "url": "https://workbench.cisecurity.org/sections/2889472/recommendations/4683055",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Binary Authorization helps to protect supply-chain security by only allowing images with verifiable cryptographically signed metadata into the cluster.",
    "rationale_statement": "Binary Authorization provides software supply-chain security for images that are deployed to GKE from Google Container Registry (GCR) or another container image registry.\nBinary Authorization requires images to be signed by trusted authorities during the development process. These signatures are then validated at deployment time. By enforcing validation, tighter control over the container environment can be gained by ensuring only verified images are integrated into the build-and-release process.",
    "impact_statement": "Care must be taken when defining policy in order to prevent inadvertent denial of container image deployments. Depending on policy, attestations for existing container images running within the cluster may need to be created before those images are redeployed or pulled as part of the pod churn.\nTo prevent key system images from being denied deployment, consider the use of global policy evaluation mode, which uses a global policy provided by Google and exempts a list of Google-provided system images from further policy evaluation.",
    "audit_procedure": "Using Google Cloud Console:\nTo check that Binary Authorization is enabled for the GKE cluster:\n\nGo to the Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list\nSelect the cluster for which Binary Authorization is disabled.\nUnder the details pane, within the Security section, ensure that 'Binary Authorization' is set to 'Enabled'.\nThen, assess the contents of the policy:\nGo to Binary Authorization by visiting: https://console.cloud.google.com/security/binary-authorization\nEnsure a policy is defined and the project default rule is not configured to 'Allow all images'.\n\nUsing Command Line:\nTo check that Binary Authorization is enabled for the GKE cluster:\ngcloud container clusters describe <cluster_name> --zone <compute_zone> --format json | jq .binaryAuthorization\n\nThe above command output will be the following if Binary Authorization is enabled:\n{\n  \"enabled\": true\n}\n\nThen, assess the contents of the policy:\ngcloud container binauthz policy export > current-policy.yaml\n\nEnsure that the current policy is not configured to allow all images (evaluationMode: ALWAYS_ALLOW):\ncat current-policy.yaml\n...\ndefaultAdmissionRule:\n  evaluationMode: ALWAYS_ALLOW",
    "remediation_procedure": "Using Google Cloud Console\n\nGo to Binary Authorization by visiting: https://console.cloud.google.com/security/binary-authorization.\nEnable the Binary Authorization API (if disabled).\nCreate an appropriate policy for use with the cluster. See https://cloud.google.com/binary-authorization/docs/policy-yaml-reference for guidance.\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\nSelect the cluster for which Binary Authorization is disabled.\nUnder the details pane, within the Security section, click on the pencil icon named Edit Binary Authorization.\nCheck the box next to Enable Binary Authorization.\nChoose Enforce policy and provide a directory for the policy to be used.\nClick SAVE CHANGES.\n\nUsing Command Line:\nUpdate the cluster to enable Binary Authorization:\ngcloud container cluster update <cluster_name> --zone <compute_zone> --binauthz-evaluation-mode=<evaluation_mode>\n\nExample:\ngcloud container clusters update $CLUSTER_NAME --zone $COMPUTE_ZONE --binauthz-evaluation-mode=PROJECT_SINGLETON_POLICY_ENFORCE\n\n\nSee: https://cloud.google.com/sdk/gcloud/reference/container/clusters/update#--binauthz-evaluation-mode for more details around the evaluation modes available.\nCreate a Binary Authorization Policy using the Binary Authorization Policy Reference: https://cloud.google.com/binary-authorization/docs/policy-yaml-reference for guidance.\nImport the policy file into Binary Authorization:\ngcloud container binauthz policy import <yaml_policy>",
    "default_value": "By default, Binary Authorization is disabled."
  },
  {
    "control_id": "2.4",
    "control": "Utilize Automated Software Inventory Tools",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889472,
    "recommendation_id": 4683056,
    "view_level": "5.10.5",
    "title": "Enable Security Posture",
    "pivot_control_id": 372,
    "pivot_recommendation_id": 4683056,
    "url": "https://workbench.cisecurity.org/sections/2889472/recommendations/4683056",
    "assessment_status": "Manual",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "",
    "rationale_statement": "The security posture dashboard provides insights about your workload security posture at the runtime phase of the software delivery life-cycle.",
    "impact_statement": "GKE security posture configuration auditing checks your workloads against a set of defined best practices.  Each configuration check has its own impact or risk.  Learn more about the checks: https://cloud.google.com/kubernetes-engine/docs/concepts/about-configuration-scanning\nExample: The host namespace check identifies pods that share host namespaces.  Pods that share host namespaces allow Pod processes to communicate with host processes and gather host information, which could lead to a container escape",
    "audit_procedure": "Check the SecurityPostureConfig on your cluster:\ngcloud container clusters --location  describe \nsecurityPostureConfig:\nmode: BASIC",
    "remediation_procedure": "Enable security posture via the UI, gCloud or API.\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/protect-workload-configuration",
    "default_value": "GKE security posture has multiple features.  Not all are on by default.  Configuration auditing is enabled by default for new standard and autopilot clusters.\nsecurityPostureConfig:\nmode: BASIC"
  },
  {
    "control_id": "2.5",
    "control": "Allowlist Authorized Software",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889458,
    "recommendation_id": 4683010,
    "view_level": "5.1.4",
    "title": "Ensure only trusted container images are used",
    "pivot_control_id": 373,
    "pivot_recommendation_id": 4683010,
    "url": "https://workbench.cisecurity.org/sections/2889458/recommendations/4683010",
    "assessment_status": "Manual",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Use Binary Authorization to allowlist (whitelist) only approved container registries.",
    "rationale_statement": "Allowing unrestricted access to external container registries provides the opportunity for malicious or unapproved containers to be deployed into the cluster. Ensuring only trusted container images are used reduces this risk.\nAlso see recommendation 5.10.4.",
    "impact_statement": "All container images to be deployed to the cluster must be hosted within an approved container image registry. If public registries are not on the allowlist, a process for bringing commonly used container images into an approved private registry and keeping them up to date will be required.",
    "audit_procedure": "Using Google Cloud Console:\nCheck that Binary Authorization is enabled for the GKE cluster:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list\nClick on the cluster and on the Details pane, ensure that Binary Authorization is set to 'Enabled'.\n\nThen assess the contents of the policy:\n\nGo to Binary Authorization by visiting: https://console.cloud.google.com/security/binary-authorization\nEnsure the project default rule is not set to 'Allow all images' under Policy deployment rules.\nReview the list of 'Images exempt from policy' for unauthorized container registries.\n\nUsing Command Line:\nCheck that Binary Authorization is enabled for the GKE cluster:\ngcloud container clusters describe <cluster_name> --zone <compute_zone> --format json | jq .binaryAuthorization\n\nThis will return the following if Binary Authorization is enabled:\n{\n  \"enabled\": true\n}\n\nThen assess the contents of the policy:\ngcloud container binauthz policy export > current-policy.yaml\n\nEnsure that the current policy is not configured to allow all images (evaluationMode: ALWAYS_ALLOW).\nReview the list of admissionWhitelistPatterns for unauthorized container registries.\ncat current-policy.yaml\nadmissionWhitelistPatterns:\n...\ndefaultAdmissionRule:\n  evaluationMode: ALWAYS_ALLOW",
    "remediation_procedure": "Using Google Cloud Console:\n\nGo to Binary Authorization by visiting: https://console.cloud.google.com/security/binary-authorization\nEnable Binary Authorization API (if disabled).\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\nSelect Kubernetes cluster for which Binary Authorization is disabled.\nWithin the Details pane, under the Security heading, click on the pencil icon called Edit binary authorization.\nEnsure that Enable Binary Authorization is checked.\nClick SAVE CHANGES.\nReturn to the Binary Authorization by visiting: https://console.cloud.google.com/security/binary-authorization.\nSet an appropriate policy for the cluster and enter the approved container registries under Image paths.\n\nUsing Command Line:\nUpdate the cluster to enable Binary Authorization:\ngcloud container cluster update <cluster_name> --enable-binauthz\n\nCreate a Binary Authorization Policy using the Binary Authorization Policy Reference: https://cloud.google.com/binary-authorization/docs/policy-yaml-reference for guidance.\nImport the policy file into Binary Authorization:\ngcloud container binauthz policy import <yaml_policy>",
    "default_value": "By default, Binary Authorization is disabled along with container registry allowlisting."
  },
  {
    "control_id": "2.5",
    "control": "Allowlist Authorized Software",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889462,
    "recommendation_id": 4683016,
    "view_level": "5.5.1",
    "title": "Ensure Container-Optimized OS (cos_containerd) is used for GKE node images",
    "pivot_control_id": 373,
    "pivot_recommendation_id": 4683016,
    "url": "https://workbench.cisecurity.org/sections/2889462/recommendations/4683016",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Use Container-Optimized OS (cos_containerd) as a managed, optimized and hardened base OS that limits the host's attack surface.",
    "rationale_statement": "COS is an operating system image for Compute Engine VMs optimized for running containers. With COS, the containers can be brought up on Google Cloud Platform quickly, efficiently, and securely.\nUsing COS as the node image provides the following benefits:\n\nRun containers out of the box: COS instances come pre-installed with the container runtime and cloud-init. With a COS instance, the container can be brought up at the same time as the VM is created, with no on-host setup required.\nSmaller attack surface: COS has a smaller footprint, reducing the instance's potential attack surface.\nLocked-down by default: COS instances include a locked-down firewall and other security settings by default.",
    "impact_statement": "If modifying an existing cluster's Node pool to run COS, the upgrade operation used is long-running and will block other operations on the cluster (including delete) until it has run to completion.\nCOS nodes also provide an option with containerd as the main container runtime directly integrated with Kubernetes instead of docker. Thus, on these nodes, Docker cannot view or access containers or images managed by Kubernetes. Applications should not interact with Docker directly. For general troubleshooting or debugging, use crictl instead.",
    "audit_procedure": "Using Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\nFrom the list of clusters, select the cluster under test.\nUnder the 'Node pools' section, make sure that for each of the Node pools, 'Container-Optimized OS (cos_containerd)' is listed in the 'Image type' column.\n\nUsing Command line:\nTo check Node image type for an existing cluster's Node pool:\ngcloud container node-pools describe <node_pool_name> --cluster <cluster_name> --zone <compute_zone> --format json | jq '.config.imageType'\n\nThe output of the above command returns COS_CONTAINERD, if COS_CONTAINERD is used for Node images.",
    "remediation_procedure": "Using Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\nSelect the Kubernetes cluster which does not use COS.\nUnder the Node pools heading, select the Node Pool that requires alteration.\nClick EDIT.\nUnder the Image Type heading click CHANGE.\nFrom the pop-up menu select Container-optimised OS with containerd (cos_containerd) (default) and click CHANGE\nRepeat for all non-compliant Node pools.\n\nUsing Command Line:\nTo set the node image to cos for an existing cluster's Node pool:\ngcloud container clusters upgrade <cluster_name> --image-type cos_containerd --zone <compute_zone> --node-pool <node_pool_name>",
    "default_value": "Container-optimised OS with containerd (cos_containerd) (default) is the default option for a cluster node image."
  },
  {
    "control_id": "3",
    "control": "Data Protection",
    "IG1": "-",
    "IG2": "-",
    "IG3": "-",
    "section_id": 2889467,
    "recommendation_id": 4683039,
    "view_level": "4.4.1",
    "title": "Prefer using secrets as files over secrets as environment variables",
    "pivot_control_id": 376,
    "pivot_recommendation_id": 4683039,
    "url": "https://workbench.cisecurity.org/sections/2889467/recommendations/4683039",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Kubernetes supports mounting secrets as data volumes or as environment variables. Minimize the use of environment variable secrets.",
    "rationale_statement": "It is reasonably common for application code to log out its environment (particularly in the event of an error). This will include any secret values passed in as environment variables, so secrets can easily be exposed to any user or entity who has access to the logs.",
    "impact_statement": "Application code which expects to read secrets in the form of environment variables would need modification",
    "audit_procedure": "Run the following command to find references to objects which use environment variables defined from secrets.\nkubectl get all -o jsonpath='{range .items[?(@..secretKeyRef)]} {.kind} {.metadata.name} {\"\\n\"}{end}' -A",
    "remediation_procedure": "If possible, rewrite application code to read secrets from mounted secret files, rather than from environment variables.",
    "default_value": "By default, secrets are not defined"
  },
  {
    "control_id": "3",
    "control": "Data Protection",
    "IG1": "-",
    "IG2": "-",
    "IG3": "-",
    "section_id": 2889467,
    "recommendation_id": 4683040,
    "view_level": "4.4.2",
    "title": "Consider external secret storage",
    "pivot_control_id": 376,
    "pivot_recommendation_id": 4683040,
    "url": "https://workbench.cisecurity.org/sections/2889467/recommendations/4683040",
    "assessment_status": "Manual",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Consider the use of an external secrets storage and management system instead of using Kubernetes Secrets directly, if more complex secret management is required. Ensure the solution requires authentication to access secrets, has auditing of access to and use of secrets, and encrypts secrets. Some solutions also make it easier to rotate secrets.",
    "rationale_statement": "Kubernetes supports secrets as first-class objects, but care needs to be taken to ensure that access to secrets is carefully limited. Using an external secrets provider can ease the management of access to secrets, especially where secrests are used across both Kubernetes and non-Kubernetes environments.",
    "impact_statement": "None",
    "audit_procedure": "Review your secrets management implementation.",
    "remediation_procedure": "Refer to the secrets management options offered by the cloud service provider or a third-party secrets management solution.",
    "default_value": "By default, no external secret management is configured."
  },
  {
    "control_id": "3.3",
    "control": "Configure Data Access Control Lists",
    "IG1": "o",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889453,
    "recommendation_id": 4682990,
    "view_level": "3.1.1",
    "title": "Ensure that the proxy kubeconfig file permissions are set to 644 or more restrictive",
    "pivot_control_id": 379,
    "pivot_recommendation_id": 4682990,
    "url": "https://workbench.cisecurity.org/sections/2889453/recommendations/4682990",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "If kube-proxy is running, and if it is configured by a kubeconfig file, ensure that the proxy kubeconfig file has permissions of 644 or more restrictive.",
    "rationale_statement": "The kube-proxy kubeconfig file controls various parameters of the kube-proxy service on the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable only by the administrators on the system.",
    "impact_statement": "Overly permissive file permissions increase security risk to the platform.",
    "audit_procedure": "Using Google Cloud Console\n\nGo to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\nClick on the desired cluster to open the Details page, then click on the desired Node pool to open the Node pool Details page\nNote the name of the desired node\nGo to VM Instances by visiting https://console.cloud.google.com/compute/instances\nFind the desired node and click on 'SSH' to open an SSH connection to the node.\n\nUsing Command Line\nMethod 1\nSSH to the worker nodes\nTo check to see if the Kubelet Service is running:\nsudo systemctl status kubelet\n\nThe output should return Active: active (running) since..\nRun the following command on each node to find the appropriate kubeconfig file:\nps -ef | grep kubelet\n\nThe output of the above command should return something similar to --kubeconfig /var/lib/kubelet/kubeconfig which is the location of the kubeconfig file.\nRun this command to obtain the kubeconfig file permissions:\nstat -c %a /var/lib/kubelet/kubeconfig\n\nThe output of the above command gives you the kubeconfig file's permissions.\nVerify that if a file is specified and it exists, the permissions are 644 or more restrictive.\nMethod 2\nCreate and Run a Privileged Pod.\nYou will need to run a pod that is privileged enough to access the host's file system. This can be achieved by deploying a pod that uses the hostPath volume to mount the node's file system into the pod.\nHere's an example of a simple pod definition that mounts the root of the host to /host within the pod:\napiVersion: v1\nkind: Pod\nmetadata:\n  name: file-check\nspec:\n  volumes:\n  - name: host-root\n    hostPath:\n      path: /\n      type: Directory\n  containers:\n  - name: nsenter\n    image: busybox\n    command: [\"sleep\", \"3600\"]\n    volumeMounts:\n    - name: host-root\n      mountPath: /host\n    securityContext:\n      privileged: true\n\nSave this to a file (e.g., file-check-pod.yaml) and create the pod:\nkubectl apply -f file-check-pod.yaml\n\nOnce the pod is running, you can exec into it to check file permissions on the node:\nkubectl exec -it file-check -- sh\n\nNow you are in a shell inside the pod, but you can access the node's file system through the /host directory and check the permission level of the file:\nls -l /host/var/lib/kubelet/kubeconfig\n\nVerify that if a file is specified and it exists, the permissions are 644 or more restrictive.",
    "remediation_procedure": "Run the below command (based on the file location on your system) on the each worker\nnode. For example,\nchmod 644 <proxy kubeconfig file>",
    "default_value": "The default permissions of the proxy kubeconfig file are 644."
  },
  {
    "control_id": "3.3",
    "control": "Configure Data Access Control Lists",
    "IG1": "o",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889453,
    "recommendation_id": 4682991,
    "view_level": "3.1.2",
    "title": "Ensure that the proxy kubeconfig file ownership is set to root:root",
    "pivot_control_id": 379,
    "pivot_recommendation_id": 4682991,
    "url": "https://workbench.cisecurity.org/sections/2889453/recommendations/4682991",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "If kube-proxy is running, ensure that the file ownership of its kubeconfig file is set to root:root.",
    "rationale_statement": "The kubeconfig file for kube-proxy controls various parameters for the kube-proxy service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by root:root.",
    "impact_statement": "Overly permissive file access increases the security risk to the platform.",
    "audit_procedure": "Using Google Cloud Console\n\nGo to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\nClick on the desired cluster to open the Details page, then click on the desired Node pool to open the Node pool Details page\nNote the name of the desired node\nGo to VM Instances by visiting https://console.cloud.google.com/compute/instances\nFind the desired node and click on 'SSH' to open an SSH connection to the node.\n\nUsing Command Line\nMethod 1\nSSH to the worker nodes\nTo check to see if the Kubelet Service is running:\nsudo systemctl status kubelet\n\nThe output should return Active: active (running) since..\nRun the following command on each node to find the appropriate kubeconfig file:\nps -ef | grep kubelet\n\nThe output of the above command should return something similar to --kubeconfig /var/lib/kubelet/kubeconfig which is the location of the kubeconfig file.\nRun this command to obtain the kubeconfig file ownership:\nstat -c %U:%G /var/lib/kubelet/kubeconfig\n\nThe output of the above command gives you the kubeconfig file's ownership. Verify that the ownership is set to root:root.\nMethod 2\nCreate and Run a Privileged Pod.\nYou will need to run a pod that is privileged enough to access the host's file system. This can be achieved by deploying a pod that uses the hostPath volume to mount the node's file system into the pod.\nHere's an example of a simple pod definition that mounts the root of the host to /host within the pod:\napiVersion: v1\nkind: Pod\nmetadata:\n  name: file-check\nspec:\n  volumes:\n  - name: host-root\n    hostPath:\n      path: /\n      type: Directory\n  containers:\n  - name: nsenter\n    image: busybox\n    command: [\"sleep\", \"3600\"]\n    volumeMounts:\n    - name: host-root\n      mountPath: /host\n    securityContext:\n      privileged: true\n\nSave this to a file (e.g., file-check-pod.yaml) and create the pod:\nkubectl apply -f file-check-pod.yaml\n\nOnce the pod is running, you can exec into it to check file ownership on the node:\nkubectl exec -it file-check -- sh\n\nNow you are in a shell inside the pod, but you can access the node's file system through the /host directory and check the ownership of the file:\nls -l /host/var/lib/kubelet/kubeconfig\n\nThe output of the above command gives you the kubeconfig file's ownership. Verify that the ownership is set to root:root.",
    "remediation_procedure": "Run the below command (based on the file location on your system) on each worker node. For example,\nchown root:root <proxy kubeconfig file>",
    "default_value": "The default ownership of the proxy kubeconfig file is root:root."
  },
  {
    "control_id": "3.3",
    "control": "Configure Data Access Control Lists",
    "IG1": "o",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889453,
    "recommendation_id": 4682992,
    "view_level": "3.1.3",
    "title": "Ensure that the kubelet configuration file has permissions set to 644",
    "pivot_control_id": 379,
    "pivot_recommendation_id": 4682992,
    "url": "https://workbench.cisecurity.org/sections/2889453/recommendations/4682992",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Ensure that if the kubelet configuration file exists, it has permissions of 644.",
    "rationale_statement": "The kubelet reads various parameters, including security settings, from a config file specified by the --config argument. If this file exists, you should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.",
    "impact_statement": "Overly permissive file access increases the security risk to the platform.",
    "audit_procedure": "Using Google Cloud Console\n\nGo to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\nClick on the desired cluster to open the Details page, then click on the desired Node pool to open the Node pool Details page\nNote the name of the desired node\nGo to VM Instances by visiting https://console.cloud.google.com/compute/instances\nFind the desired node and click on 'SSH' to open an SSH connection to the node.\n\nUsing Command Line\nMethod 1\nFirst, SSH to the relevant worker node:\nTo check to see if the Kubelet Service is running:\nsudo systemctl status kubelet\n\nThe output should return Active: active (running) since..\nRun the following command on each node to find the appropriate Kubelet config file:\nps -ef | grep kubelet\n\nThe output of the above command should return something similar to --config /etc/kubernetes/kubelet-config.yaml which is the location of the Kubelet config file.\nRun the following command:\nstat -c %a /etc/kubernetes/kubelet-config.yaml\n\nThe output of the above command is the Kubelet config file's permissions. Verify that the permissions are 644 or more restrictive.\nMethod 2\nCreate and Run a Privileged Pod.\nYou will need to run a pod that is privileged enough to access the host's file system. This can be achieved by deploying a pod that uses the hostPath volume to mount the node's file system into the pod.\nHere's an example of a simple pod definition that mounts the root of the host to /host within the pod:\napiVersion: v1\nkind: Pod\nmetadata:\n  name: file-check\nspec:\n  volumes:\n  - name: host-root\n    hostPath:\n      path: /\n      type: Directory\n  containers:\n  - name: nsenter\n    image: busybox\n    command: [\"sleep\", \"3600\"]\n    volumeMounts:\n    - name: host-root\n      mountPath: /host\n    securityContext:\n      privileged: true\n\nSave this to a file (e.g., file-check-pod.yaml) and create the pod:\nkubectl apply -f file-check-pod.yaml\n\nOnce the pod is running, you can exec into it to check file permissions on the node:\nkubectl exec -it file-check -- sh\n\nNow you are in a shell inside the pod, but you can access the node's file system through the /host directory and check the permission level of the file:\nls -l /host/etc/kubernetes/kubelet-config.yaml\n\nVerify that if a file is specified and it exists, the permissions are 644 or more restrictive.",
    "remediation_procedure": "Run the following command (using the kubelet config file location):\nchmod 644 <kubelet_config_file>",
    "default_value": "The default permissions for the kubelet configuration file are 600."
  },
  {
    "control_id": "3.3",
    "control": "Configure Data Access Control Lists",
    "IG1": "o",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889453,
    "recommendation_id": 4682994,
    "view_level": "3.1.4",
    "title": "Ensure that the kubelet configuration file ownership is set to root:root",
    "pivot_control_id": 379,
    "pivot_recommendation_id": 4682994,
    "url": "https://workbench.cisecurity.org/sections/2889453/recommendations/4682994",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Ensure that if the kubelet configuration file exists, it is owned by root:root.",
    "rationale_statement": "The kubelet reads various parameters, including security settings, from a config file specified by the --config argument. If this file is specified you should restrict its file permissions to maintain the integrity of the file. The file should be owned by root:root.",
    "impact_statement": "Overly permissive file access increases the security risk to the platform.",
    "audit_procedure": "Using Google Cloud Console\n\nGo to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\nClick on the desired cluster to open the Details page, then click on the desired Node pool to open the Node pool Details page\nNote the name of the desired node\nGo to VM Instances by visiting https://console.cloud.google.com/compute/instances\nFind the desired node and click on 'SSH' to open an SSH connection to the node.\n\nUsing Command Line\nMethod 1\nFirst, SSH to the relevant worker node:\nTo check to see if the Kubelet Service is running:\nsudo systemctl status kubelet\n\nThe output should return Active: active (running) since..\nRun the following command on each node to find the appropriate Kubelet config file:\nps -ef | grep kubelet\n\nThe output of the above command should return something similar to --config /etc/kubernetes/kubelet/kubelet-config.yaml which is the location of the Kubelet config file.\nRun the following command:\nstat -c %U:%G /etc/kubernetes/kubelet/kubelet-config.yaml\n\nThe output of the above command is the Kubelet config file's ownership. Verify that the ownership is set to root:root\nMethod 2\nCreate and Run a Privileged Pod.\nYou will need to run a pod that is privileged enough to access the host's file system. This can be achieved by deploying a pod that uses the hostPath volume to mount the node's file system into the pod.\nHere's an example of a simple pod definition that mounts the root of the host to /host within the pod:\napiVersion: v1\nkind: Pod\nmetadata:\n  name: file-check\nspec:\n  volumes:\n  - name: host-root\n    hostPath:\n      path: /\n      type: Directory\n  containers:\n  - name: nsenter\n    image: busybox\n    command: [\"sleep\", \"3600\"]\n    volumeMounts:\n    - name: host-root\n      mountPath: /host\n    securityContext:\n      privileged: true\n\nSave this to a file (e.g., file-check-pod.yaml) and create the pod:\nkubectl apply -f file-check-pod.yaml\n\nOnce the pod is running, you can exec into it to check file ownership on the node:\nkubectl exec -it file-check -- sh\n\nNow you are in a shell inside the pod, but you can access the node's file system through the /host directory and check the ownership of the file:\nls -l /etc/kubernetes/kubelet/kubelet-config.yaml\n\nThe output of the above command gives you the file's ownership. Verify that the ownership is set to root:root.",
    "remediation_procedure": "Run the following command (using the config file location identified in the Audit step):\nchown root:root <kubelet_config_file>",
    "default_value": "The default file ownership is root:root."
  },
  {
    "control_id": "3.3",
    "control": "Configure Data Access Control Lists",
    "IG1": "o",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889458,
    "recommendation_id": 4683008,
    "view_level": "5.1.2",
    "title": "Minimize user access to Container Image repositories",
    "pivot_control_id": 379,
    "pivot_recommendation_id": 4683008,
    "url": "https://workbench.cisecurity.org/sections/2889458/recommendations/4683008",
    "assessment_status": "Manual",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Note: GCR is now deprecated, see the references for more details.\nRestrict user access to GCR or AR, limiting interaction with build images to only authorized personnel and service accounts.",
    "rationale_statement": "Weak access control to GCR or AR may allow malicious users to replace built images with vulnerable or back-doored containers.",
    "impact_statement": "Care should be taken not to remove access to GCR or AR for accounts that require this for their operation.\nAny account granted the Storage Object Viewer role at the project level can view all objects stored in GCS for the project.",
    "audit_procedure": "For Images Hosted in AR:\n\nGo to Artifacts Browser by visiting https://console.cloud.google.com/artifacts\nFrom the list of artifacts select each repository with format Docker\nUnder the Permissions tab, review the roles for each member and ensure only authorized users have the Artifact Registry Administrator, Artifact Registry Reader, Artifact Registry Repository Administrator and Artifact Registry Writer roles.\n\nUsers may have permissions to use Service Accounts and thus Users could inherit privileges on the AR repositories. To check the accounts that could do this:\n\nGo to IAM by visiting https://console.cloud.google.com/iam-admin/iam\nApply the filter Role: Service Account User.\n\nNote that other privileged project level roles will have the ability to write and modify AR repositories. Consult the GCP CIS benchmark and IAM documentation for further reference.\nUsing Command Line:\ngcloud artifacts repositories get-iam-policy <repository-name> --location <repository-location>\n\nThe output of the command will return roles associated with the AR repository and which members have those roles.\nFor Images Hosted in GCR:\nUsing Google Cloud Console:\nGCR bucket permissions\n\nGo to Storage Browser by visiting https://console.cloud.google.com/storage/browser\nFrom the list of storage buckets, select artifacts.<project_id>.appspot.com for the GCR bucket\nUnder the Permissions tab, review the roles for each member and ensure only authorized users have the Storage Admin, Storage Object Admin, Storage Object Creator, Storage Legacy Bucket Owner, Storage Legacy Bucket Writer and Storage Legacy Object Owner roles.\n\nUsers may have permissions to use Service Accounts and thus Users could inherit privileges on the GCR Bucket. To check the accounts that could do this:\n\nGo to IAM by visiting https://console.cloud.google.com/iam-admin/iam\nApply the filter Role: Service Account User.\n\nNote that other privileged project level roles will have the ability to write and modify objects and the GCR bucket. Consult the GCP CIS benchmark and IAM documentation for further reference.\nUsing Command Line:\nTo check GCR bucket specific permissions\ngsutil iam get gs://artifacts.<project_id>.appspot.com\n\nThe output of the command will return roles associated with the GCR bucket and which members have those roles.\nAdditionally, run the following to identify users and service accounts that hold privileged roles at the project level, and thus inherit these privileges within the GCR bucket:\ngcloud projects get-iam-policy <project_id> \\\n--flatten=\"bindings[].members\" \\\n--format='table(bindings.members,bindings.role)' \\\n--filter=\"bindings.role:roles/storage.admin OR bindings.role:roles/storage.objectAdmin OR bindings.role:roles/storage.objectCreator OR bindings.role:roles/storage.legacyBucketOwner OR bindings.role:roles/storage.legacyBucketWriter OR bindings.role:roles/storage.legacyObjectOwner\"\n\nThe output from the command lists the service accounts that have create/modify permissions.\nUsers may have permissions to use Service Accounts and thus Users could inherit privileges on the GCR Bucket. To check the accounts that could do this:\ngcloud projects get-iam-policy <project_id>  \\\n--flatten=\"bindings[].members\" \\\n--format='table(bindings.members)' \\\n--filter=\"bindings.role:roles/iam.serviceAccountUser\"\n\nNote that other privileged project level roles will have the ability to write and modify objects and the GCR bucket. Consult the GCP CIS benchmark and IAM documentation for further reference.",
    "remediation_procedure": "For Images Hosted in AR:\nUsing Google Cloud Console:\n\nGo to Artifacts Browser by visiting https://console.cloud.google.com/artifacts\nFrom the list of artifacts select each repository with format Docker\nUnder the Permissions tab, modify the roles for each member and ensure only authorized users have the Artifact Registry Administrator, Artifact Registry Reader, Artifact Registry Repository Administrator and Artifact Registry Writer roles.\n\nUsing Command Line:\ngcloud artifacts repositories set-iam-policy <repository-name> <path-to-policy-file> --location <repository-location>\n\nTo learn how to configure policy files see: https://cloud.google.com/artifact-registry/docs/access-control#grant\nFor Images Hosted in GCR:\nUsing Google Cloud Console:\nTo modify roles granted at the GCR bucket level:\n\nGo to Storage Browser by visiting: https://console.cloud.google.com/storage/browser.\nFrom the list of storage buckets, select artifacts.<project_id>.appspot.com for the GCR bucket\nUnder the Permissions tab, modify permissions of the identified member via the drop-down role menu and change the Role to Storage Object Viewer for read-only access.\n\nFor a User or Service account with Project level permissions inherited by the GCR bucket, or the Service Account User Role:\n\nGo to IAM by visiting: https://console.cloud.google.com/iam-admin/iam\nFind the User or Service account to be modified and click on the corresponding pencil icon.\nRemove the create/modify role (Storage Admin / Storage Object Admin / Storage Object Creator / Service Account User) on the user or service account.\nIf required add the Storage Object Viewer role - note with caution that this permits the account to view all objects stored in GCS for the project.\n\nUsing Command Line:\nTo change roles at the GCR bucket level:\nFirstly, run the following if read permissions are required:\ngsutil iam ch <type>:<email_address>:objectViewer gs://artifacts.<project_id>.appspot.com\n\nThen remove the excessively privileged role (Storage Admin / Storage Object Admin / Storage Object Creator) using:\ngsutil iam ch -d <type>:<email_address>:<role> gs://artifacts.<project_id>.appspot.com\n\nwhere:\n\n<type> can be one of the following:\n\nuser, if the <email_address> is a Google account.\nserviceAccount, if <email_address> specifies a Service account.\n<email_address> can be one of the following:\n\na Google account (for example, [email\u00a0protected]).\na Cloud IAM service account.\n\n\n\n\n\nTo modify roles defined at the project level and subsequently inherited within the GCR bucket, or the Service Account User role, extract the IAM policy file, modify it accordingly and apply it using:\ngcloud projects set-iam-policy <project_id> <policy_file>",
    "default_value": "By default, GCR is disabled and access controls are set during initialisation."
  },
  {
    "control_id": "3.3",
    "control": "Configure Data Access Control Lists",
    "IG1": "o",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889458,
    "recommendation_id": 4683009,
    "view_level": "5.1.3",
    "title": "Minimize cluster access to read-only for Container Image repositories",
    "pivot_control_id": 379,
    "pivot_recommendation_id": 4683009,
    "url": "https://workbench.cisecurity.org/sections/2889458/recommendations/4683009",
    "assessment_status": "Manual",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Note: GCR is now deprecated, see the references for more details.\nConfigure the Cluster Service Account with Artifact Registry Viewer Role to only allow read-only access to AR repositories.\nConfigure the Cluster Service Account with Storage Object Viewer Role to only allow read-only access to GCR.",
    "rationale_statement": "The Cluster Service Account does not require administrative access to GCR or AR, only requiring pull access to containers to deploy onto GKE. Restricting permissions follows the principles of least privilege and prevents credentials from being abused beyond the required role.",
    "impact_statement": "A separate dedicated service account may be required for use by build servers and other robot users pushing or managing container images.\nAny account granted the Storage Object Viewer role at the project level can view all objects stored in GCS for the project.",
    "audit_procedure": "For Images Hosted in AR:\nUsing Google Cloud Console\n\nGo to Artifacts Browser by visiting https://console.cloud.google.com/artifacts\nFrom the list of repositories, for each repository with Format Docker\nUnder the Permissions tab, review the role for GKE Service account and ensure that only the Artifact Registry Viewer role is set.\n\nUsing Command Line:\ngcloud artifacts repositories get-iam-policy <repository-name> --location <repository-location>\n\nThe output of the command will return roles associated with the AR repository. If listed, ensure the GKE Service account is set to \"role\": \"roles/artifactregistry.reader\".\nFor Images Hosted in GCR:\nUsing Google Cloud Console\n\nGo to Storage Browser by visiting https://console.cloud.google.com/storage/browser\nFrom the list of storage buckets, select artifacts.<project_id>.appspot.com for the GCR bucket\nUnder the Permissions tab, review the role for GKE Service account and ensure that only the Storage Object Viewer role is set.\n\nUsing Command Line\nGCR bucket permissions\ngsutil iam get gs://artifacts.<project_id>.appspot.com\n\nThe output of the command will return roles associated with the GCR bucket. If listed, ensure the GKE Service account is set to \"role\": \"roles/storage.objectViewer\".\nIf the GKE Service Account has project level permissions that are inherited within the bucket, ensure that these are not privileged:\ngcloud projects get-iam-policy <project_id> \\\n--flatten=\"bindings[].members\" \\\n--format='table(bindings.members,bindings.role)' \\\n--filter=\"bindings.role:roles/storage.admin OR bindings.role:roles/storage.objectAdmin OR bindings.role:roles/storage.objectCreator OR bindings.role:roles/storage.legacyBucketOwner OR bindings.role:roles/storage.legacyBucketWriter OR bindings.role:roles/storage.legacyObjectOwner\"\n\nYour GKE Service Account should not be output when this command is run.",
    "remediation_procedure": "For Images Hosted in AR:\nUsing Google Cloud Console:\n\nGo to Artifacts Browser by visiting https://console.cloud.google.com/artifacts\nFrom the list of repositories, for each repository with Format Docker\nUnder the Permissions tab, modify the permissions for GKE Service account and ensure that only the Artifact Registry Viewer role is set.\n\nUsing Command Line:\nAdd artifactregistry.reader role\ngcloud artifacts repositories add-iam-policy-binding <repository> \\\n--location=<repository-location> \\\n--member='serviceAccount:<email-address>' \\\n--role='roles/artifactregistry.reader'\n\nRemove any roles other than artifactregistry.reader\ngcloud artifacts repositories remove-iam-policy-binding <repository> \\\n--location <repository-location> \\\n--member='serviceAccount:<email-address>' \\\n--role='<role-name>'\n\nFor Images Hosted in GCR:\nUsing Google Cloud Console:\nFor an account explicitly granted access to the bucket:\n\nGo to Storage Browser by visiting: https://console.cloud.google.com/storage/browser.\nFrom the list of storage buckets, select artifacts.<project_id>.appspot.com for the GCR bucket.\nUnder the Permissions tab, modify permissions of the identified GKE Service Account via the drop-down role menu and change to the Role to Storage Object Viewer for read-only access.\n\nFor an account that inherits access to the bucket through Project level permissions:\n\nGo to IAM console by visiting: https://console.cloud.google.com/iam-admin.\nFrom the list of accounts, identify the required service account and select the corresponding pencil icon.\nRemove the Storage Admin / Storage Object Admin / Storage Object Creator roles.\nAdd the Storage Object Viewer role - note with caution that this permits the account to view all objects stored in GCS for the project.\nClick SAVE.\n\nUsing Command Line:\nFor an account explicitly granted to the bucket:\nFirstly add read access to the Kubernetes Service Account:\ngsutil iam ch <type>:<email_address>:objectViewer gs://artifacts.<project_id>.appspot.com\n\nwhere:\n\n<type> can be one of the following:\n\nuser, if the <email_address> is a Google account.\nserviceAccount, if <email_address> specifies a Service account.\n<email_address> can be one of the following:\n\na Google account (for example, [email\u00a0protected]).\na Cloud IAM service account.\n\n\n\n\n\nThen remove the excessively privileged role (Storage Admin / Storage Object Admin / Storage Object Creator) using:\ngsutil iam ch -d <type>:<email_address>:<role> gs://artifacts.<project_id>.appspot.com\n\nFor an account that inherits access to the GCR Bucket through Project level permissions, modify the Projects IAM policy file accordingly, then upload it using:\ngcloud projects set-iam-policy <project_id> <policy_file>",
    "default_value": "The default permissions for the cluster Service account is dependent on the initial configuration and IAM policy."
  },
  {
    "control_id": "3.3",
    "control": "Configure Data Access Control Lists",
    "IG1": "o",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889463,
    "recommendation_id": 4683030,
    "view_level": "5.6.3",
    "title": "Ensure Control Plane Authorized Networks is Enabled",
    "pivot_control_id": 379,
    "pivot_recommendation_id": 4683030,
    "url": "https://workbench.cisecurity.org/sections/2889463/recommendations/4683030",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Enable Control Plane Authorized Networks to restrict access to the cluster's control plane to only an allowlist of authorized IPs.",
    "rationale_statement": "Authorized networks are a way of specifying a restricted range of IP addresses that are permitted to access your cluster's control plane. Kubernetes Engine uses both Transport Layer Security (TLS) and authentication to provide secure access to your cluster's control plane from the public internet. This provides you the flexibility to administer your cluster from anywhere; however, you might want to further restrict access to a set of IP addresses that you control. You can set this restriction by specifying an authorized network.\nControl Plane Authorized Networks blocks untrusted IP addresses. Google Cloud Platform IPs (such as traffic from Compute Engine VMs) can reach your master through HTTPS provided that they have the necessary Kubernetes credentials.\nRestricting access to an authorized network can provide additional security benefits for your container cluster, including:\n\nBetter protection from outsider attacks: Authorized networks provide an additional layer of security by limiting external, non-GCP access to a specific set of addresses you designate, such as those that originate from your premises. This helps protect access to your cluster in the case of a vulnerability in the cluster's authentication or authorization mechanism.\nBetter protection from insider attacks: Authorized networks help protect your cluster from accidental leaks of master certificates from your company's premises. Leaked certificates used from outside GCP and outside the authorized IP ranges (for example, from addresses outside your company) are still denied access.",
    "impact_statement": "When implementing Control Plane Authorized Networks, be careful to ensure all desired networks are on the allowlist to prevent inadvertently blocking external access to your cluster's control plane.",
    "audit_procedure": "Using Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list\nFrom the list of clusters, click on the cluster to open the Details page and make sure 'Master authorized networks' is set to 'Enabled'.\n\nUsing Command Line:\nTo check Master Authorized Networks status for an existing cluster, run the following command;\ngcloud container clusters update $CLUSTER_NAME --zone $COMPUTE_ZONE --enable-master-authorized-networks\n\n\nThe output should return\n{\n  \"enabled\": true\n}\n\nif Control Plane Authorized Networks is enabled. If Master Authorized Networks is disabled, the above command will return null ({ }).",
    "remediation_procedure": "Using Google Cloud Console:\n\nGo to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\nSelect Kubernetes clusters for which Control Plane Authorized Networks is disabled\nWithin the Details pane, under the Networking heading, click on the pencil icon named Edit control plane authorised networks.\nCheck the box next to Enable control plane authorised networks.\nClick SAVE CHANGES.\n\nUsing Command Line:\nTo enable Control Plane Authorized Networks for an existing cluster, run the following command:\ngcloud container clusters update <cluster_name> --zone <compute_zone> --enable-master-authorized-networks\n\nAlong with this, you can list authorized networks using the --master-authorized-networks flag which contains a list of up to 20 external networks that are allowed to connect to your cluster's control plane through HTTPS. You provide these networks as a comma-separated list of addresses in CIDR notation (such as 90.90.100.0/24).",
    "default_value": "By default, Control Plane Authorized Networks is disabled."
  },
  {
    "control_id": "3.10",
    "control": "Encrypt Sensitive Data in Transit",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889456,
    "recommendation_id": 4682997,
    "view_level": "3.2.3",
    "title": "Ensure that a Client CA File is Configured",
    "pivot_control_id": 528,
    "pivot_recommendation_id": 4682997,
    "url": "https://workbench.cisecurity.org/sections/2889456/recommendations/4682997",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Enable Kubelet authentication using certificates.",
    "rationale_statement": "The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet\u2019s port-forwarding functionality. These connections terminate at the kubelet\u2019s HTTPS endpoint. By default, the apiserver does not verify the kubelet\u2019s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Enabling Kubelet certificate authentication ensures that the apiserver could authenticate the Kubelet before submitting any requests.",
    "impact_statement": "You require TLS to be configured on apiserver as well as kubelets.",
    "audit_procedure": "Audit Method 1:\nKubelets can accept configuration via a configuration file and in some cases via command line arguments. It is important to note that parameters provided as command line arguments will override their counterpart parameters in the configuration file (see --config details in the Kubelet CLI Reference for more info, where you can also find out which configuration parameters can be supplied as a command line argument).\nWith this in mind, it is important to check for the existence of command line arguments as well as configuration file entries when auditing Kubelet configuration.\nFirstly, SSH to each node and execute the following command to find the Kubelet process:\nps -ef | grep kubelet\n\nThe output of the above command provides details of the active Kubelet process, from which we can see the command line arguments provided to the process. Also note the location of the configuration file, provided with the --config argument, as this will be needed to verify configuration. The file can be viewed with a command such as more or less, like so:\nsudo less /path/to/kubelet-config.json\n\nVerify that a client certificate authority file is configured. This may be configured using a command line argument to the kubelet service with --client-ca-file or in the kubelet configuration file via \"authentication\": { \"x509\": {\"clientCAFile\": <path/to/client-ca-file> } }\".\nAudit Method 2:\nIt is also possible to review the running configuration of a Kubelet via the /configz endpoint of the Kubernetes API. This can be achieved using kubectl to proxy your requests to the API.\nDiscover all nodes in your cluster by running the following command:\nkubectl get nodes\n\nNext, initiate a proxy with kubectl on a local port of your choice. In this example we will use 8080:\nkubectl proxy --port=8080\n\nWith this running, in a separate terminal run the following command for each node:\nexport NODE_NAME=my-node-name\ncurl http://localhost:8080/api/v1/nodes/${NODE_NAME}/proxy/configz    \n\nThe curl command will return the API response which will be a JSON formatted string representing the Kubelet configuration.\nVerify that a client certificate authority file is configured with \"authentication\": { \"x509\": {\"clientCAFile\": <path/to/client-ca-file> } }\" in the API response.",
    "remediation_procedure": "Remediation Method 1:\nIf configuring via the Kubelet config file, you first need to locate the file.\nTo do this, SSH to each node and execute the following command to find the kubelet process:\nps -ef | grep kubelet\n\nThe output of the above command provides details of the active kubelet process, from which we can see the location of the configuration file provided to the kubelet service with the --config argument. The file can be viewed with a command such as more or less, like so:\nsudo less /path/to/kubelet-config.json\n\nConfigure the client certificate authority file by setting the following parameter appropriately:\n\"authentication\": { \"x509\": {\"clientCAFile\": <path/to/client-ca-file> } }\"\n\nRemediation Method 2:\nIf using executable arguments, edit the kubelet service file on each worker node and ensure the below parameters are part of the KUBELET_ARGS variable string.\nFor systems using systemd, such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, then this file can be found at /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf. Otherwise, you may need to look up documentation for your chosen operating system to determine which service manager is configured:\n--client-ca-file=<path/to/client-ca-file>\n\nFor Both Remediation Steps:\nBased on your system, restart the kubelet service and check the service status.\nThe following example is for operating systems using systemd, such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, and invokes the systemctl command. If systemctl is not available then you will need to look up documentation for your chosen operating system to determine which service manager is configured:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l",
    "default_value": "See the GKE documentation for the default value."
  },
  {
    "control_id": "3.10",
    "control": "Encrypt Sensitive Data in Transit",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889456,
    "recommendation_id": 4683004,
    "view_level": "3.2.8",
    "title": "Ensure that the --rotate-certificates argument is not present or is set to true",
    "pivot_control_id": 528,
    "pivot_recommendation_id": 4683004,
    "url": "https://workbench.cisecurity.org/sections/2889456/recommendations/4683004",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Enable kubelet client certificate rotation.",
    "rationale_statement": "The --rotate-certificates setting causes the kubelet to rotate its client certificates by creating new CSRs as its existing credentials expire. This automated periodic rotation ensures that the there is no downtime due to expired certificates and thus addressing availability in the CIA (Confidentiality, Integrity, and Availability) security triad.\nNote: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to implement rotation yourself.\nNote: This feature also requires the RotateKubeletClientCertificate feature gate to be enabled.",
    "impact_statement": "None",
    "audit_procedure": "Audit Method 1:\nSSH to each node and run the following command to find the Kubelet process:\nps -ef | grep kubelet\n\nIf the output of the command above includes the --RotateCertificate executable argument, verify that it is set to true.\nIf the output of the command above does not include the --RotateCertificate executable argument then check the Kubelet config file. The output of the above command should return something similar to --config /etc/kubernetes/kubelet/kubelet-config.json which is the location of the Kubelet config file.\nOpen the Kubelet config file:\ncat /etc/kubernetes/kubelet-config.yaml\n\nVerify that the RotateCertificate argument is not present, or is set to true.",
    "remediation_procedure": "Remediation Method 1:\nIf modifying the Kubelet config file, edit the kubelet-config.yaml file /etc/kubernetes/kubelet/kubelet-config.yaml and set the below parameter to true\n\"RotateCertificate\":true\n\nAdditionally, ensure that the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf does not set the --RotateCertificate executable argument to false because this would override the Kubelet config file.\nRemediation Method 2:\nIf using executable arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf on each worker node and add the below parameter at the end of the KUBELET_ARGS variable string.\n--RotateCertificate=true",
    "default_value": "See the GKE documentation for the default value."
  },
  {
    "control_id": "3.10",
    "control": "Encrypt Sensitive Data in Transit",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889456,
    "recommendation_id": 4683005,
    "view_level": "3.2.9",
    "title": "Ensure that the RotateKubeletServerCertificate argument is set to true",
    "pivot_control_id": 528,
    "pivot_recommendation_id": 4683005,
    "url": "https://workbench.cisecurity.org/sections/2889456/recommendations/4683005",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Enable kubelet server certificate rotation.",
    "rationale_statement": "RotateKubeletServerCertificate causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA (Confidentiality, Integrity, and Availability) security triad.\nNote: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to implement rotation yourself.",
    "impact_statement": "None",
    "audit_procedure": "Audit Method 1:\nFirst, SSH to each node:\nRun the following command on each node to find the Kubelet process:\nps -ef | grep kubelet\n\nIf the output of the command above includes the --rotate-kubelet-server-certificate executable argument verify that it is set to true.\nIf the process does not have the --rotate-kubelet-server-certificate executable argument then check the Kubelet config file. The output of the above command should return something similar to --config /etc/kubernetes/kubelet-config.yaml which is the location of the Kubelet config file.\nOpen the Kubelet config file:\ncat /etc/kubernetes/kubelet-config.yaml\n\nVerify that RotateKubeletServerCertificate argument exists in the featureGates section and is set to true.\nAudit Method 2:\nIf using the api configz endpoint consider searching for the status of \"RotateKubeletServerCertificate\":true by extracting the live configuration from the nodes running kubelet.\nSet the local proxy port and the following variables and provide proxy port number and node name;\nHOSTNAME_PORT=\"localhost-and-port-number\"\nNODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\"\nkubectl proxy --port=8001 &\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=gke-cluster-1-pool1-5e572947-r2hg (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"",
    "remediation_procedure": "Remediation Method 1:\nIf modifying the Kubelet config file, edit the kubelet-config.json file /etc/kubernetes/kubelet-config.yaml and set the below parameter to true\n\"featureGates\": {\n  \"RotateKubeletServerCertificate\":true\n},\n\nAdditionally, ensure that the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf does not set the --rotate-kubelet-server-certificate executable argument to false because this would override the Kubelet config file.\nRemediation Method 2:\nIf using executable arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf on each worker node and add the below parameter at the end of the KUBELET_ARGS variable string.\n--rotate-kubelet-server-certificate=true\n\nRemediation Method 3:\nIf using the api configz endpoint consider searching for the status of \"RotateKubeletServerCertificate\": by extracting the live configuration from the nodes running kubelet.\n**See detailed step-by-step configmap procedures in Reconfigure a Node's Kubelet in a Live Cluster, and then rerun the curl statement from audit process to check for kubelet configuration changes\nkubectl proxy --port=8001 &\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=gke-cluster-1-pool1-5e572947-r2hg (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\nFor all three remediation methods:\nRestart the kubelet service and check status. The example below is for when using systemctl to manage services:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l",
    "default_value": "See the GKE documentation for the default value."
  },
  {
    "control_id": "3.10",
    "control": "Encrypt Sensitive Data in Transit",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889463,
    "recommendation_id": 4683037,
    "view_level": "5.6.7",
    "title": "Ensure use of Google-managed SSL Certificates",
    "pivot_control_id": 528,
    "pivot_recommendation_id": 4683037,
    "url": "https://workbench.cisecurity.org/sections/2889463/recommendations/4683037",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Encrypt traffic to HTTPS load balancers using Google-managed SSL certificates.",
    "rationale_statement": "Encrypting traffic between users and the Kubernetes workload is fundamental to protecting data sent over the web.\nGoogle-managed SSL Certificates are provisioned, renewed, and managed for domain names. This is only available for HTTPS load balancers created using Ingress Resources, and not TCP/UDP load balancers created using Service of type:LoadBalancer.",
    "impact_statement": "Google-managed SSL Certificates are less flexible than certificates that are self obtained and managed. Managed certificates support a single, non-wildcard domain. Self-managed certificates can support wildcards and multiple subject alternative names (SANs).",
    "audit_procedure": "Using Command Line:\nIdentify if there are any workloads exposed publicly using Services of type:LoadBalancer:\nkubectl get svc -A -o json | jq '.items[] | select(.spec.type==\"LoadBalancer\")'\n\nConsider using ingresses instead of these services in order to use Google managed SSL certificates.\nFor the ingresses within the cluster, run the following command:\nkubectl get ingress -A -o json | jq .items[] | jq '{name: .metadata.name, annotations: .metadata.annotations, namespace: .metadata.namespace, status: .status}'\n\nThe above command should return the name of the ingress, namespace, annotations and status. Check that the following annotation is present to ensure managed certificates are referenced.\n\"annotations\": {\n    ...\n    \"networking.gke.io/managed-certificates\": \"<example_certificate>\"\n  },\n\nFor completeness, run the following command to ensure that the managed certificate resource exists:\nkubectl get managedcertificates -A \n\nThe above command returns a list of managed certificates for which <example_certificate> should exist within the same namespace as the ingress.",
    "remediation_procedure": "If services of type:LoadBalancer are discovered, consider replacing the Service with an Ingress.\nTo configure the Ingress and use Google-managed SSL certificates, follow the instructions as listed at: https://cloud.google.com/kubernetes-engine/docs/how-to/managed-certs.",
    "default_value": "By default, Google-managed SSL Certificates are not created when an Ingress resource is defined."
  },
  {
    "control_id": "3.11",
    "control": "Encrypt Sensitive Data at Rest",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889460,
    "recommendation_id": 4683013,
    "view_level": "5.3.1",
    "title": "Ensure Kubernetes Secrets are encrypted using keys managed in Cloud KMS",
    "pivot_control_id": 386,
    "pivot_recommendation_id": 4683013,
    "url": "https://workbench.cisecurity.org/sections/2889460/recommendations/4683013",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Encrypt Kubernetes secrets, stored in etcd, at the application-layer using a customer-managed key in Cloud KMS.",
    "rationale_statement": "By default, GKE encrypts customer content stored at rest, including Secrets. GKE handles and manages this default encryption for you without any additional action on your part.\nApplication-layer Secrets Encryption provides an additional layer of security for sensitive data, such as user defined Secrets and Secrets required for the operation of the cluster, such as service account keys, which are all stored in etcd.\nUsing this functionality, you can use a key, that you manage in Cloud KMS, to encrypt data at the application layer. This protects against attackers in the event that they manage to gain access to etcd.",
    "impact_statement": "To use the Cloud KMS CryptoKey to protect etcd in the cluster, the 'Kubernetes Engine Service Agent' Service account must hold the 'Cloud KMS CryptoKey Encrypter/Decrypter' role.",
    "audit_procedure": "Using Google Cloud Console:\n\nGo to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\nFrom the list of clusters, click on each cluster to bring up the Details pane, and ensure Application-layer Secrets Encryption is set to 'Enabled'.\n\nUsing Command Line:\ngcloud container clusters describe $CLUSTER_NAME --zone $COMPUTE_ZONE --format json | jq '.databaseEncryption'\n\nIf configured correctly, the output from the command returns a response containing the following detail:\nkeyName=projects/<key_project_id>/locations/<location>/keyRings/<ring_name>/cryptoKeys/<key_name>]\nstate=ENCRYPTED\n\n{\n  \"currentState\": \"CURRENT_STATE_ENCRYPTED\",\n  \"keyName\": \"projects/<key_project_id>/locations/us-central1/keyRings/<ring_name>/cryptoKeys/<key_name>\",\n  \"state\": \"ENCRYPTED\"\n}",
    "remediation_procedure": "To enable Application-layer Secrets Encryption, several configuration items are required. These include:\n\nA key ring\nA key\nA GKE service account with Cloud KMS CryptoKey Encrypter/Decrypter role\n\nOnce these are created, Application-layer Secrets Encryption can be enabled on an existing or new cluster.\nUsing Google Cloud Console:\nTo create a key\n\nGo to Cloud KMS by visiting https://console.cloud.google.com/security/kms.\nSelect CREATE KEY RING.\nEnter a Key ring name and the region where the keys will be stored.\nClick CREATE.\nEnter a Key name and appropriate rotation period within the Create key pane.\nClick CREATE.\n\nTo enable on a new cluster\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\nClick CREATE CLUSTER, and choose the required cluster mode.\nWithin the Security heading, under CLUSTER, check Encrypt secrets at the application layer checkbox.\nSelect the kms key as the customer-managed key and, if prompted, grant permissions to the GKE Service account.\nClick CREATE.\n\nTo enable on an existing cluster\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\nSelect the cluster to be updated.\nUnder the Details pane, within the Security heading, click on the pencil named Application-layer secrets encryption.\nEnable Encrypt secrets at the application layer and choose a kms key.\nClick SAVE CHANGES.\n\nUsing Command Line:\nTo create a key:\nCreate a key ring:\ngcloud kms keyrings create <ring_name> --location <location> --project <key_project_id>\n\nCreate a key:\ngcloud kms keys create <key_name> --location <location> --keyring <ring_name> --purpose encryption --project <key_project_id>\n\nGrant the Kubernetes Engine Service Agent service account the Cloud KMS CryptoKey Encrypter/Decrypter role:\ngcloud kms keys add-iam-policy-binding <key_name> --location <location> --keyring <ring_name> --member serviceAccount:<service_account_name> --role roles/cloudkms.cryptoKeyEncrypterDecrypter --project <key_project_id>\n\nTo create a new cluster with Application-layer Secrets Encryption:\ngcloud container clusters create <cluster_name> --cluster-version=latest --zone <zone> --database-encryption-key projects/<key_project_id>/locations/<location>/keyRings/<ring_name>/cryptoKeys/<key_name> --project <cluster_project_id>\n\nTo enable on an existing cluster:\ngcloud container clusters update <cluster_name> --zone <zone> --database-encryption-key projects/<key_project_id>/locations/<location>/keyRings/<ring_name>/cryptoKeys/<key_name> --project <cluster_project_id>",
    "default_value": "By default, Application-layer Secrets Encryption is disabled."
  },
  {
    "control_id": "3.11",
    "control": "Encrypt Sensitive Data at Rest",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889471,
    "recommendation_id": 4683048,
    "view_level": "5.9.1",
    "title": "Enable Customer-Managed Encryption Keys (CMEK) for GKE Persistent Disks (PD)",
    "pivot_control_id": 386,
    "pivot_recommendation_id": 4683048,
    "url": "https://workbench.cisecurity.org/sections/2889471/recommendations/4683048",
    "assessment_status": "Manual",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Use Customer-Managed Encryption Keys (CMEK) to encrypt dynamically-provisioned attached Google Compute Engine Persistent Disks (PDs) using keys managed within Cloud Key Management Service (Cloud KMS).",
    "rationale_statement": "GCE persistent disks are encrypted at rest by default using envelope encryption with keys managed by Google. For additional protection, users can manage the Key Encryption Keys using Cloud KMS.",
    "impact_statement": "Encryption of dynamically-provisioned attached disks requires the use of the self-provisioned Compute Engine Persistent Disk CSI Driver v0.5.1 or higher.\nIf CMEK is being configured with a regional cluster, the cluster must run GKE 1.14 or higher.",
    "audit_procedure": "Using Google Cloud Console:\n\nGo to Compute Engine Disks by visiting: https://console.cloud.google.com/compute/disks\nSelect each disk used by the cluster, and ensure the Encryption Type is listed as Customer Managed.\n\nUsing Command Line:\nIdentify the Persistent Volumes Used by the cluster:\nkubectl get pv -o json | jq '.items[].metadata.name'\n\nFor each volume used, check that it is encrypted using a customer managed key by running the following command:\ngcloud compute disks describe <pv_name> --zone <compute_zone> --format json | jq '.diskEncryptionKey.kmsKeyName'\n\nThis returns null ({ }) if a customer-managed encryption key is not used to encrypt the disk.",
    "remediation_procedure": "This cannot be remediated by updating an existing cluster. The node pool must either be recreated or a new cluster created.\nUsing Google Cloud Console:\nThis is not possible using Google Cloud Console.\nUsing Command Line:\nFollow the instructions detailed at: https://cloud.google.com/kubernetes-engine/docs/how-to/using-cmek.",
    "default_value": "Persistent disks are encrypted at rest by default, but are not encrypted using Customer-Managed Encryption Keys by default. By default, the Compute Engine Persistent Disk CSI Driver is not provisioned within the cluster."
  },
  {
    "control_id": "3.11",
    "control": "Encrypt Sensitive Data at Rest",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889471,
    "recommendation_id": 4683050,
    "view_level": "5.9.2",
    "title": "Enable Customer-Managed Encryption Keys (CMEK) for Boot Disks",
    "pivot_control_id": 386,
    "pivot_recommendation_id": 4683050,
    "url": "https://workbench.cisecurity.org/sections/2889471/recommendations/4683050",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Use Customer-Managed Encryption Keys (CMEK) to encrypt node boot disks using keys managed within Cloud Key Management Service (Cloud KMS).",
    "rationale_statement": "GCE persistent disks are encrypted at rest by default using envelope encryption with keys managed by Google. For additional protection, users can manage the Key Encryption Keys using Cloud KMS.",
    "impact_statement": "Encryption of dynamically-provisioned attached disks requires the use of the self-provisioned Compute Engine Persistent Disk CSI Driver v0.5.1 or higher.\nIf CMEK is being configured with a regional cluster, the cluster must run GKE 1.14 or higher.",
    "audit_procedure": "Using Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list\nClick on each cluster, and click on any Node pools\nOn the Node pool Details page, under the Security heading, check that Boot disk encryption type is set to Customer managed with the desired key.\n\nUsing Command Line:\nRun this command:\ngcloud container node-pools describe $NODE_POOL --cluster $CLUSTER_NAME --zone $COMPUTE_ZONE\n\nVerify that the output of the above command includes a diskType of either pd-standard, pd-balanced or pd-ssd, and the bootDiskKmsKey is specified as the desired key.",
    "remediation_procedure": "This cannot be remediated by updating an existing cluster. The node pool must either be recreated or a new cluster created.\nUsing Google Cloud Console:\nTo create a new node pool:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list\nSelect Kubernetes clusters for which node boot disk CMEK is disabled.\nClick ADD NODE POOL.\nIn the Nodes section, under machine configuration, ensure Boot disk type is Standard persistent disk or SSD persistent disk.\nSelect Enable customer-managed encryption for Boot Disk and select the Cloud KMS encryption key to be used.\nClick CREATE.\n\nTo create a new cluster:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list\nClick CREATE and click `CONFIGURE for the required cluster mode.\nUnder NODE POOLS, expand the default-pool list and click Nodes.\nIn the Configure node settings pane, select Standard persistent disk or SSD Persistent Disk as the Boot disk type.\nSelect Enable customer-managed encryption for Boot Disk check box and choose the Cloud KMS encryption key to be used.\nConfigure the rest of the cluster settings as required.\nClick CREATE.\n\nUsing Command Line:\nCreate a new node pool using customer-managed encryption keys for the node boot disk, of <disk_type> either pd-standard or pd-ssd:\ngcloud container node-pools create <cluster_name> --disk-type <disk_type> --boot-disk-kms-key projects/<key_project_id>/locations/<location>/keyRings/<ring_name>/cryptoKeys/<key_name>\n\nCreate a cluster using customer-managed encryption keys for the node boot disk, of <disk_type> either pd-standard or pd-ssd:\ngcloud container clusters create <cluster_name> --disk-type <disk_type> --boot-disk-kms-key projects/<key_project_id>/locations/<location>/keyRings/<ring_name>/cryptoKeys/<key_name>",
    "default_value": "Persistent disks are encrypted at rest by default, but are not encrypted using Customer-Managed Encryption Keys by default. By default, the Compute Engine Persistent Disk CSI Driver is not provisioned within the cluster."
  },
  {
    "control_id": "4",
    "control": "Secure Configuration of Enterprise Assets and Software",
    "IG1": "-",
    "IG2": "-",
    "IG3": "-",
    "section_id": 2889470,
    "recommendation_id": 4683049,
    "view_level": "4.6.3",
    "title": "Apply Security Context to Pods and Containers",
    "pivot_control_id": 390,
    "pivot_recommendation_id": 4683049,
    "url": "https://workbench.cisecurity.org/sections/2889470/recommendations/4683049",
    "assessment_status": "Manual",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Apply Security Context to Pods and Containers",
    "rationale_statement": "A security context defines the operating system security settings (uid, gid, capabilities, SELinux role, etc..) applied to a container. When designing containers and pods, make sure that the security context is configured for pods, containers, and volumes. A security context is a property defined in the deployment yaml. It controls the security parameters that will be assigned to the pod/container/volume. There are two levels of security context: pod level security context, and container level security context.",
    "impact_statement": "If you incorrectly apply security contexts, there may be issues running the pods.",
    "audit_procedure": "Review the pod definitions in the cluster and verify that the security contexts have been defined as appropriate.",
    "remediation_procedure": "Follow the Kubernetes documentation and apply security contexts to your pods. For a suggested list of security contexts, you may refer to the CIS Google Container-Optimized OS Benchmark.",
    "default_value": "By default, no security contexts are automatically applied to pods."
  },
  {
    "control_id": "4.1",
    "control": "Establish and Maintain a Secure Configuration Process",
    "IG1": "o",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889455,
    "recommendation_id": 4682999,
    "view_level": "4.1.2",
    "title": "Minimize access to secrets",
    "pivot_control_id": 391,
    "pivot_recommendation_id": 4682999,
    "url": "https://workbench.cisecurity.org/sections/2889455/recommendations/4682999",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "The Kubernetes API stores secrets, which may be service account tokens for the Kubernetes API or credentials used by workloads in the cluster.  Access to these secrets should be restricted to the smallest possible group of users to reduce the risk of privilege escalation.",
    "rationale_statement": "Inappropriate access to secrets stored within the Kubernetes cluster can allow for an attacker to gain additional access to the Kubernetes cluster or external resources whose credentials are stored as secrets.",
    "impact_statement": "Care should be taken not to remove access to secrets to system components which require this for their operation",
    "audit_procedure": "Review the users who have get, list or watch access to secrets objects in the Kubernetes API.",
    "remediation_procedure": "Where possible, remove get, list and watch access to secret objects in the cluster.",
    "default_value": "CLUSTERROLEBINDING                                    SUBJECT                             TYPE            SA-NAMESPACE\ncluster-admin                                         system:masters                      Group           \nsystem:controller:clusterrole-aggregation-controller  clusterrole-aggregation-controller  ServiceAccount  kube-system\nsystem:controller:expand-controller                   expand-controller                   ServiceAccount  kube-system\nsystem:controller:generic-garbage-collector           generic-garbage-collector           ServiceAccount  kube-system\nsystem:controller:namespace-controller                namespace-controller                ServiceAccount  kube-system\nsystem:controller:persistent-volume-binder            persistent-volume-binder            ServiceAccount  kube-system\nsystem:kube-controller-manager                        system:kube-controller-manager      User"
  },
  {
    "control_id": "4.4",
    "control": "Implement and Manage a Firewall on Servers",
    "IG1": "o",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889463,
    "recommendation_id": 4683031,
    "view_level": "5.6.4",
    "title": "Ensure clusters are created with Private Endpoint Enabled and Public Access Disabled",
    "pivot_control_id": 394,
    "pivot_recommendation_id": 4683031,
    "url": "https://workbench.cisecurity.org/sections/2889463/recommendations/4683031",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Disable access to the Kubernetes API from outside the node network if it is not required.",
    "rationale_statement": "In a private cluster, the master node has two endpoints, a private and public endpoint. The private endpoint is the internal IP address of the master, behind an internal load balancer in the master's VPC network. Nodes communicate with the master using the private endpoint. The public endpoint enables the Kubernetes API to be accessed from outside the master's VPC network.\nAlthough Kubernetes API requires an authorized token to perform sensitive actions, a vulnerability could potentially expose the Kubernetes publically with unrestricted access. Additionally, an attacker may be able to identify the current cluster and Kubernetes API version and determine whether it is vulnerable to an attack. Unless required, disabling public endpoint will help prevent such threats, and require the attacker to be on the master's VPC network to perform any attack on the Kubernetes API.",
    "impact_statement": "To enable a Private Endpoint, the cluster has to also be configured with private nodes, a private master IP range and IP aliasing enabled.\nIf the Private Endpoint flag --enable-private-endpoint is passed to the gcloud CLI, or the external IP address undefined in the Google Cloud Console during cluster creation, then all access from a public IP address is prohibited.",
    "audit_procedure": "Using Google Cloud Console:\n\nGo to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\nSelect the required cluster, and within the Details pane, make sure the 'Endpoint' does not have a public IP address.\n\nUsing Command Line:\nRun this command:\ngcloud container clusters describe <cluster_name> --format json | jq '.privateClusterConfig.enablePrivateEndpoint'\n\nThe output of the above command returns true if a Private Endpoint is enabled with Public Access disabled.\nFor an additional check, the endpoint parameter can be queried with the following command:\ngcloud container clusters describe <cluster_name> --format json | jq '.endpoint'\n\nThe output of the above command returns a private IP address if Private Endpoint is enabled with Public Access disabled.",
    "remediation_procedure": "Once a cluster is created without enabling Private Endpoint only, it cannot be remediated. Rather, the cluster must be recreated.\nUsing Google Cloud Console:\n\nGo to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\nClick CREATE CLUSTER, and choose CONFIGURE for the Standard mode cluster.\nConfigure the cluster as required then click Networking under CLUSTER in the navigation pane.\nUnder IPv4 network access, click the Private cluster radio button.\nUncheck the Access control plane using its external IP address checkbox.\nIn the Control plane IP range textbox, provide an IP range for the control plane.\nConfigure the other settings as required, and click CREATE.\n\nUsing Command Line:\nCreate a cluster with a Private Endpoint enabled and Public Access disabled by including the --enable-private-endpoint flag within the cluster create command:\ngcloud container clusters create <cluster_name> --enable-private-endpoint\n\nSetting this flag also requires the setting of --enable-private-nodes, --enable-ip-alias and --master-ipv4-cidr=<master_cidr_range>.",
    "default_value": "By default, the Private Endpoint is disabled."
  },
  {
    "control_id": "4.4",
    "control": "Implement and Manage a Firewall on Servers",
    "IG1": "o",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889463,
    "recommendation_id": 4683033,
    "view_level": "5.6.5",
    "title": "Ensure clusters are created with Private Nodes",
    "pivot_control_id": 394,
    "pivot_recommendation_id": 4683033,
    "url": "https://workbench.cisecurity.org/sections/2889463/recommendations/4683033",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Private Nodes are nodes with no public IP addresses. Disable public IP addresses for cluster nodes, so that they only have private IP addresses.",
    "rationale_statement": "Disabling public IP addresses on cluster nodes restricts access to only internal networks, forcing attackers to obtain local network access before attempting to compromise the underlying Kubernetes hosts.",
    "impact_statement": "To enable Private Nodes, the cluster has to also be configured with a private master IP range and IP Aliasing enabled.\nPrivate Nodes do not have outbound access to the public internet. If you want to provide outbound Internet access for your private nodes, you can use Cloud NAT or you can manage your own NAT gateway.\nTo access Google Cloud APIs and services from private nodes, Private Google Access needs to be set on Kubernetes Engine Cluster Subnets.",
    "audit_procedure": "Using Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\nSelect the desired cluster, and within the Details pane, make sure Private Clusters is set to Enabled.\n\nUsing Command Line:\nRun this command:\ngcloud container clusters describe <cluster_name> --format json | jq '.privateClusterConfig.enablePrivateNodes'\n\nThe output of the above command returns true if Private Nodes is enabled.",
    "remediation_procedure": "Once a cluster is created without enabling Private Nodes, it cannot be remediated. Rather the cluster must be recreated.\nUsing Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\nClick CREATE CLUSTER.\nConfigure the cluster as required then click Networking under CLUSTER in the navigation pane.\nUnder IPv4 network access, click the Private cluster radio button.\nConfigure the other settings as required, and click CREATE.\n\nUsing Command Line:\nTo create a cluster with Private Nodes enabled, include the --enable-private-nodes flag within the cluster create command:\ngcloud container clusters create <cluster_name> --enable-private-nodes\n\nSetting this flag also requires the setting of --enable-ip-alias and --master-ipv4-cidr=<master_cidr_range>.",
    "default_value": "By default, Private Nodes are disabled."
  },
  {
    "control_id": "4.4",
    "control": "Implement and Manage a Firewall on Servers",
    "IG1": "o",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889463,
    "recommendation_id": 4683036,
    "view_level": "5.6.6",
    "title": "Consider firewalling GKE worker nodes",
    "pivot_control_id": 394,
    "pivot_recommendation_id": 4683036,
    "url": "https://workbench.cisecurity.org/sections/2889463/recommendations/4683036",
    "assessment_status": "Manual",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Reduce the network attack surface of GKE nodes by using Firewalls to restrict ingress and egress traffic.",
    "rationale_statement": "Utilizing stringent ingress and egress firewall rules minimizes the ports and services exposed to an network-based attacker, whilst also restricting egress routes within or out of the cluster in the event that a compromised component attempts to form an outbound connection.",
    "impact_statement": "All instances targeted by a firewall rule, either using a tag or a service account will be affected. Ensure there are no adverse effects on other instances using the target tag or service account before implementing the firewall rule.",
    "audit_procedure": "Using Google Cloud Console:\n\nGo to Compute Engine by visiting: https://console.cloud.google.com/compute/instances.\nFor each instance within your cluster, use the 'more actions' menu (3 vertical dots) and select to 'View network details'.\nIf there are multiple network interfaces attached to the instance, select the network interface to view in the 'Network interface' details section and see all the rules that apply to the network interface, within the 'Firewall rules' tab. Make sure the firewall rules are appropriate for your environment.\n\nUsing Command Line:\nFor the instance being evaluated, obtain its Service account and tags:\ngcloud compute instances describe <instance_name> --zone <compute_zone> --format json | jq '{tags: .tags.items[], serviceaccount:.serviceAccounts[].email, network: .networkInterfaces[].network}'\n\nThis will return:\n{\n  \"tags\": \"<tag>\",\n  \"serviceaccount\": \"<service_account>\"\n  \"network\": \"https://www.googleapis.com/compute/v1/projects/<project_id>/global/networks/<network>\"\n}\n\nThen, observe the firewall rules applied to the instance by using the following command, replacing <tag> and <service_account> as appropriate:\ngcloud compute firewall-rules list \\\n  --format=\"table(\n                name,\n                network,\n                direction,\n                priority,\n                sourceRanges.list():label=SRC_RANGES,\n                destinationRanges.list():label=DEST_RANGES,\n                allowed[].map().firewall_rule().list():label=ALLOW,\n                denied[].map().firewall_rule().list():label=DENY,\n                sourceTags.list():label=SRC_TAGS,\n                sourceServiceAccounts.list():label=SRC_SVC_ACCT,\n                targetTags.list():label=TARGET_TAGS,\n                targetServiceAccounts.list():label=TARGET_SVC_ACCT,\n                disabled\n            )\" \\\n  --filter=\"targetTags.list():<tag> OR targetServiceAccounts.list():<service_account>\"\n\nFirewall rules may also be applied to a network without specifically targeting Tags or Service Accounts. These can be observed using the following, replacing <network> as appropriate:\ngcloud compute firewall-rules list \\\n  --format=\"table(\n                name,\n                network,\n                direction,\n                priority,\n                sourceRanges.list():label=SRC_RANGES,\n                destinationRanges.list():label=DEST_RANGES,\n                allowed[].map().firewall_rule().list():label=ALLOW,\n                denied[].map().firewall_rule().list():label=DENY,\n                sourceTags.list():label=SRC_TAGS,\n                sourceServiceAccounts.list():label=SRC_SVC_ACCT,\n                targetTags.list():label=TARGET_TAGS,\n                targetServiceAccounts.list():label=TARGET_SVC_ACCT,\n                disabled\n            )\" \\\n  --filter=\"network.list():<network> AND -targetTags.list():* AND -targetServiceAccounts.list():*\"",
    "remediation_procedure": "Using Google Cloud Console:\n\nGo to Firewall Rules by visiting: https://console.cloud.google.com/networking/firewalls/list\nClick CREATE FIREWALL RULE.\nConfigure the firewall rule as required. Ensure the firewall targets the nodes correctly, either selecting the nodes using tags (under Targets, select Specified target tags, and set Target tags to <tag>), or using the Service account associated with node (under Targets, select Specified service account, set Service account scope as appropriate, and Target service account to <service_account>).\nClick CREATE.\n\nUsing Command Line:\nUse the following command to generate firewall rules, setting the variables as appropriate:\ngcloud compute firewall-rules create <firewall_rule_name> --network <network> --priority <priority> --direction <direction> --action <action> --target-tags <tag> --target-service-accounts <service_account> --source-ranges <source_cidr_range> --source-tags <source_tags> --source-service-accounts <source_service_account> --destination-ranges <destination_cidr_range> --rules <rules>",
    "default_value": "Every VPC network has two implied firewall rules. These rules exist, but are not shown in the Cloud Console:\n\nThe implied allow egress rule: An egress rule whose action is allow, destination is 0.0.0.0/0, and priority is the lowest possible (65535) lets any instance send traffic to any destination, except for traffic blocked by GCP. Outbound access may be restricted by a higher priority firewall rule. Internet access is allowed if no other firewall rules deny outbound traffic and if the instance has an external IP address or uses a NAT instance.\nThe implied deny ingress rule: An ingress rule whose action is deny, source is 0.0.0.0/0, and priority is the lowest possible (65535) protects all instances by blocking incoming traffic to them. Incoming access may be allowed by a higher priority rule. Note that the default network includes some additional rules that override this one, allowing certain types of incoming traffic.\n\nThe implied rules cannot be removed, but they have the lowest possible priorities."
  },
  {
    "control_id": "4.6",
    "control": "Securely Manage Enterprise Assets and Software",
    "IG1": "o",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889469,
    "recommendation_id": 4683043,
    "view_level": "4.5.1",
    "title": "Configure Image Provenance using ImagePolicyWebhook admission controller",
    "pivot_control_id": 396,
    "pivot_recommendation_id": 4683043,
    "url": "https://workbench.cisecurity.org/sections/2889469/recommendations/4683043",
    "assessment_status": "Manual",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Configure Image Provenance for the deployment.",
    "rationale_statement": "Kubernetes supports plugging in provenance rules to accept or reject the images in deployments. Rules can be configured to ensure that only approved images are deployed in the cluster.\nAlso see recommendation 5.10.4.",
    "impact_statement": "Regular maintenance for the provenance configuration should be carried out, based on container image updates.",
    "audit_procedure": "Review the pod definitions in the cluster and verify that image provenance is configured as appropriate.\nAlso see recommendation 5.10.4.",
    "remediation_procedure": "Follow the Kubernetes documentation and setup image provenance.\nAlso see recommendation 5.10.4.",
    "default_value": "By default, image provenance is not set."
  },
  {
    "control_id": "4.7",
    "control": "Manage Default Accounts on Enterprise Assets and Software",
    "IG1": "o",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889459,
    "recommendation_id": 4683011,
    "view_level": "5.2.1",
    "title": "Ensure GKE clusters are not running using the Compute Engine default service account",
    "pivot_control_id": 397,
    "pivot_recommendation_id": 4683011,
    "url": "https://workbench.cisecurity.org/sections/2889459/recommendations/4683011",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Create and use minimally privileged Service accounts to run GKE cluster nodes instead of using the Compute Engine default Service account. Unnecessary permissions could be abused in the case of a node compromise.",
    "rationale_statement": "A GCP service account (as distinct from a Kubernetes ServiceAccount) is an identity that an instance or an application can be used to run GCP API requests. This identity is used to identify virtual machine instances to other Google Cloud Platform services. By default, Kubernetes Engine nodes use the Compute Engine default service account. This account has broad access by default, as defined by access scopes, making it useful to a wide variety of applications on the VM, but it has more permissions than are required to run your Kubernetes Engine cluster.\nA minimally privileged service account should be created and used to run the Kubernetes Engine cluster instead of using the Compute Engine default service account, and create separate service accounts for each Kubernetes Workload (See recommendation 5.2.2).\nKubernetes Engine requires, at a minimum, the node service account to have the monitoring.viewer, monitoring.metricWriter, and logging.logWriter roles. Additional roles may need to be added for the nodes to pull images from GCR.",
    "impact_statement": "Instances are automatically granted the https://www.googleapis.com/auth/cloud-platform scope to allow full access to all Google Cloud APIs. This is so that the IAM permissions of the instance are completely determined by the IAM roles of the Service account. Thus if Kubernetes workloads were using cluster access scopes to perform actions using Google APIs, they may no longer be able to, if not permitted by the permissions of the Service account. To remediate, follow recommendation 5.2.2.\nThe Service account roles listed here are the minimum required to run the cluster. Additional roles may be required to pull from a private instance of Google Container Registry (GCR).",
    "audit_procedure": "Using Google Cloud Console:\n\nGo to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\nSelect the cluster under test and click on each Node pool to bring up the Node pool details page. Ensure that for each Node pool the Service account is not set to default under the Security heading.\n\nTo check the permissions allocated to the service account are the minimum required for cluster operation:\n\nGo to IAM by visiting https://console.cloud.google.com/iam-admin/iam\nFrom the list of Service accounts, ensure each cluster Service account has only the following roles:\n\n\nLogs Writer\nMonitoring Metric Writer\nMonitoring Viewer\n\nUsing Command line:\nTo check which Service account is set for an existing cluster, run the following command:\ngcloud container node-pools describe $NODE_POOL --cluster $CLUSTER_NAME --zone $COMPUTE_ZONE --format json | jq '.config.serviceAccount'\n\nThe output of the above command will return default if default Service account is used for Project access.\nTo check that the permissions allocated to the service account are the minimum required for cluster operation:\ngcloud projects get-iam-policy <project_id> \\\n  --flatten=\"bindings[].members\" \\\n  --format='table(bindings.role)' \\\n  --filter=\"bindings.members:<service_account>\"\n\nReview the output to ensure that the service account only has the roles required to run the cluster:\n\nroles/logging.logWriter\nroles/monitoring.metricWriter\nroles/monitoring.viewer",
    "remediation_procedure": "Using Google Cloud Console:\nTo create a minimally privileged service account:\n\nGo to Service Accounts by visiting: https://console.cloud.google.com/iam-admin/serviceaccounts.\nClick on CREATE SERVICE ACCOUNT.\nEnter Service Account Details.\nClick CREATE AND CONTINUE.\nWithin Service Account permissions add the following roles:\n\nLogs Writer.\nMonitoring Metric Writer.\n`Monitoring Viewer.\n\n\nClick CONTINUE.\nGrant users access to this service account and create keys as required.\nClick DONE.\n\nTo create a Node pool to use the Service account:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\nClick on the cluster name within which the Node pool will be launched.\nClick on ADD NODE POOL.\nWithin the Node Pool details, select the Security subheading, and under `Identity defaults, select the minimally privileged service account from the Service Account drop-down.\nClick `CREATE to launch the Node pool.\n\nNote: The workloads will need to be migrated to the new Node pool, and the old node pools that use the default service account should be deleted to complete the remediation.\nUsing Command Line:\nTo create a minimally privileged service account:\ngcloud iam service-accounts create <node_sa_name> --display-name \"GKE Node Service Account\"\nexport NODE_SA_EMAIL=gcloud iam service-accounts list --format='value(email)' --filter='displayName:GKE Node Service Account'\n\nGrant the following roles to the service account:\nexport PROJECT_ID=gcloud config get-value project\ngcloud projects add-iam-policy-binding <project_id> --member serviceAccount:<node_sa_email> --role roles/monitoring.metricWriter\ngcloud projects add-iam-policy-binding <project_id> --member serviceAccount:<node_sa_email> --role roles/monitoring.viewer\ngcloud projects add-iam-policy-binding <project_id> --member serviceAccount:<node_sa_email> --role roles/logging.logWriter\n\nTo create a new Node pool using the Service account, run the following command:\ngcloud container node-pools create <node_pool> --service-account=<sa_name>@<project_id>.iam.gserviceaccount.com--cluster=<cluster_name> --zone <compute_zone>\n\nNote: The workloads will need to be migrated to the new Node pool, and the old node pools that use the default service account should be deleted to complete the remediation.",
    "default_value": "By default, nodes use the Compute Engine default service account when you create a new cluster."
  },
  {
    "control_id": "4.7",
    "control": "Manage Default Accounts on Enterprise Assets and Software",
    "IG1": "o",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889459,
    "recommendation_id": 4683012,
    "view_level": "5.2.2",
    "title": "Prefer using dedicated GCP Service Accounts and Workload Identity",
    "pivot_control_id": 397,
    "pivot_recommendation_id": 4683012,
    "url": "https://workbench.cisecurity.org/sections/2889459/recommendations/4683012",
    "assessment_status": "Manual",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Kubernetes workloads should not use cluster node service accounts to authenticate to Google Cloud APIs. Each Kubernetes Workload that needs to authenticate to other Google services using Cloud IAM should be provisioned a dedicated Service account. Enabling Workload Identity manages the distribution and rotation of Service account keys for the workloads to use.",
    "rationale_statement": "Manual approaches for authenticating Kubernetes workloads running on GKE against Google Cloud APIs are: storing service account keys as a Kubernetes secret (which introduces manual key rotation and potential for key compromise); or use of the underlying nodes' IAM Service account, which violates the principle of least privilege on a multitenanted node,  when one pod needs to have access to a service, but every other pod on the node that uses the Service account does not.\nOnce a relationship between a Kubernetes Service account and a GCP Service account has been configured, any workload running as the Kubernetes Service account automatically authenticates as the mapped GCP Service account when accessing Google Cloud APIs on a cluster with Workload Identity enabled.",
    "impact_statement": "Workload Identity replaces the need to use Metadata Concealment and as such, the two approaches are incompatible. The sensitive metadata protected by Metadata Concealment is also protected by Workload Identity.\nWhen Workload Identity is enabled, the Compute Engine default Service account can not be used. Correspondingly, Workload Identity can't be used with Pods running in the host network. Workloads may also need to be modified in order for them to use Workload Identity, as described within: https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity\nGKE infrastructure pods such as Stackdriver will continue to use the Node's Service account.",
    "audit_procedure": "Using Google Cloud Console:\n\nGo to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\nFrom the list of clusters, click on each cluster to bring up the Details pane, make sure for each cluster Workload Identity is set to 'Enabled' under the 'Cluster' section and ensure that the Workload Identity Namespace is set to the namespace of the GCP project containing the cluster, for example: <project_id>.svc.id.goog\nAdditionally, click on each Node pool within each cluster to observe the Node pool Details pane, and ensure that the GKE Metadata Server is 'Enabled'.\n\nUsing Command Line:\ngcloud container clusters describe <cluster_name> --zone <cluster_zone>\n\nIf Workload Identity is enabled, the following fields should be present, and the <project_id> should be set to the namespace of the GCP project containing the cluster:\nworkloadIdentityConfig:\n  identityNamespace:<project_id>.svc.id.goog\n\nFor each Node pool, ensure the following is set.\nworkloadMetadataConfig:\n    nodeMetadata: GKE_METADATA_SERVER\n\nEach Kubernetes workload requiring Google Cloud API access will need to be manually audited to ensure that Workload Identity is being used and not some other method.",
    "remediation_procedure": "Using Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\nFrom the list of clusters, select the cluster for which Workload Identity is disabled.\nWithin the Details pane, under the Security section, click on the pencil icon named Edit workload identity.\nEnable Workload Identity and set the workload pool to the namespace of the Cloud project containing the cluster, for example: <project_id>.svc.id.goog.\nClick SAVE CHANGES and wait for the cluster to update.\nOnce the cluster has updated, select each Node pool within the cluster Details page.\nFor each Node pool, select EDIT within the Node pool Details page\nWithin the Edit node pool pane, check the 'Enable GKE Metadata Server' checkbox and click SAVE.\n\nUsing Command Line:\ngcloud container clusters update <cluster_name> --zone <cluster_zone> --workload-pool <project_id>.svc.id.goog\n\nNote that existing Node pools are unaffected. New Node pools default to --workload-metadata-from-node=GKE_METADATA_SERVER.\nThen, modify existing Node pools to enable GKE_METADATA_SERVER:\ngcloud container node-pools update <node_pool_name> --cluster <cluster_name> --zone <cluster_zone> --workload-metadata=GKE_METADATA\n\nWorkloads may need to be modified in order for them to use Workload Identity as described within: https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity. Also consider the effects on the availability of hosted workloads as Node pools are updated. It may be more appropriate to create new Node Pools.",
    "default_value": "By default, Workload Identity is disabled."
  },
  {
    "control_id": "4.8",
    "control": "Uninstall or Disable Unnecessary Services on Enterprise Assets and Software",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889455,
    "recommendation_id": 4683015,
    "view_level": "4.1.5",
    "title": "Ensure that Service Account Tokens are only mounted where necessary",
    "pivot_control_id": 398,
    "pivot_recommendation_id": 4683015,
    "url": "https://workbench.cisecurity.org/sections/2889455/recommendations/4683015",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Service accounts tokens should not be mounted in pods except where the workload running in the pod explicitly needs to communicate with the API server",
    "rationale_statement": "Mounting service account tokens inside pods can provide an avenue for privilege escalation attacks where an attacker is able to compromise a single pod in the cluster.\nAvoiding mounting these tokens removes this attack avenue.",
    "impact_statement": "Pods mounted without service account tokens will not be able to communicate with the API server, except where the resource is available to unauthenticated principals.",
    "audit_procedure": "Review pod and service account objects in the cluster and ensure that the option below is set, unless the resource explicitly requires this access.\nautomountServiceAccountToken: false",
    "remediation_procedure": "Modify the definition of pods and service accounts which do not need to mount service account tokens to disable it.",
    "default_value": "By default, all pods get a service account token mounted in them."
  },
  {
    "control_id": "4.8",
    "control": "Uninstall or Disable Unnecessary Services on Enterprise Assets and Software",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889472,
    "recommendation_id": 4683052,
    "view_level": "5.10.1",
    "title": "Ensure Kubernetes Web UI is Disabled",
    "pivot_control_id": 398,
    "pivot_recommendation_id": 4683052,
    "url": "https://workbench.cisecurity.org/sections/2889472/recommendations/4683052",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Note: The Kubernetes web UI (Dashboard) does not have admin access by default in GKE 1.7 and higher. The Kubernetes web UI is disabled by default in GKE 1.10 and higher. In GKE 1.15 and higher, the Kubernetes web UI add-on KubernetesDashboard is no longer supported as a managed add-on.\nThe Kubernetes Web UI (Dashboard) has been a historical source of vulnerability and should only be deployed when necessary.",
    "rationale_statement": "You should disable the Kubernetes Web UI (Dashboard) when running on Kubernetes Engine. The Kubernetes Web UI is backed by a highly privileged Kubernetes Service Account.\nThe Google Cloud Console provides all the required functionality of the Kubernetes Web UI and leverages Cloud IAM to restrict user access to sensitive cluster controls and settings.",
    "impact_statement": "Users will be required to manage cluster resources using the Google Cloud Console or the command line. These require appropriate permissions. To use the command line, this requires the installation of the command line client, kubectl, on the user's device (this is already included in Cloud Shell) and knowledge of command line operations.",
    "audit_procedure": "Using Google Cloud Console:\nCurrently not possible, due to the add-on having been removed. Must use the command line.\nUsing Command Line:\nRun the following command:\ngcloud container clusters describe <cluster_name> --zone <compute_zone> --format json | jq '.addonsConfig.kubernetesDashboard'\n\nEnsure the output of the above command has JSON key attribute disabled set to true:\n{\n    \"disabled\": true\n}",
    "remediation_procedure": "Using Google Cloud Console:\nCurrently not possible, due to the add-on having been removed. Must use the command line.\nUsing Command Line:\nTo disable the Kubernetes Dashboard on an existing cluster, run the following command:\ngcloud container clusters update <cluster_name> --zone <zone> --update-addons=KubernetesDashboard=DISABLED",
    "default_value": "The Kubernetes web UI (Dashboard) does not have admin access by default in GKE 1.7 and higher. The Kubernetes web UI is disabled by default in GKE 1.10 and higher. In GKE 1.15 and higher, the Kubernetes web UI add-on KubernetesDashboard is no longer supported as a managed add-on."
  },
  {
    "control_id": "5.2",
    "control": "Use Unique Passwords",
    "IG1": "o",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889455,
    "recommendation_id": 4683001,
    "view_level": "4.1.3",
    "title": "Minimize wildcard use in Roles and ClusterRoles",
    "pivot_control_id": 404,
    "pivot_recommendation_id": 4683001,
    "url": "https://workbench.cisecurity.org/sections/2889455/recommendations/4683001",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Kubernetes Roles and ClusterRoles provide access to resources based on sets of objects and actions that can be taken on those objects.  It is possible to set either of these to be the wildcard \"*\", which matches all items.\nUse of wildcards is not optimal from a security perspective as it may allow for inadvertent access to be granted when new resources are added to the Kubernetes API either as CRDs or in later versions of the product.",
    "rationale_statement": "The principle of least privilege recommends that users are provided only the access required for their role and nothing more.  The use of wildcard rights grants is likely to provide excessive rights to the Kubernetes API.",
    "impact_statement": "",
    "audit_procedure": "Retrieve the roles defined across each namespaces in the cluster and review for wildcards\nkubectl get roles --all-namespaces -o yaml\n\nRetrieve the cluster roles defined in the cluster and review for wildcards\nkubectl get clusterroles -o yaml",
    "remediation_procedure": "Where possible replace any use of wildcards in clusterroles and roles with specific objects or actions.",
    "default_value": ""
  },
  {
    "control_id": "5.3",
    "control": "Disable Dormant Accounts",
    "IG1": "o",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889456,
    "recommendation_id": 4682995,
    "view_level": "3.2.1",
    "title": "Ensure that the Anonymous Auth is Not Enabled Draft",
    "pivot_control_id": 405,
    "pivot_recommendation_id": 4682995,
    "url": "https://workbench.cisecurity.org/sections/2889456/recommendations/4682995",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Disable anonymous requests to the Kubelet server.",
    "rationale_statement": "When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the Kubelet server. You should rely on authentication to authorize access and disallow anonymous requests.",
    "impact_statement": "Anonymous requests will be rejected.",
    "audit_procedure": "Audit Method 1:\nKubelets can accept configuration via a configuration file and in some cases via command line arguments. It is important to note that parameters provided as command line arguments will override their counterpart parameters in the configuration file (see --config details in the Kubelet CLI Reference for more info, where you can also find out which configuration parameters can be supplied as a command line argument).\nWith this in mind, it is important to check for the existence of command line arguments as well as configuration file entries when auditing Kubelet configuration.\nFirstly, SSH to each node and execute the following command to find the Kubelet process:\nps -ef | grep kubelet\n\nThe output of the above command provides details of the active Kubelet process, from which we can see the command line arguments provided to the process. Also note the location of the configuration file, provided with the --config argument, as this will be needed to verify configuration. The file can be viewed with a command such as more or less, like so:\nsudo less /path/to/kubelet-config.json\n\nVerify that Anonymous Authentication is not enabled. This may be configured as a command line argument to the kubelet service with --anonymous-auth=false or in the kubelet configuration file via \"authentication\": { \"anonymous\": { \"enabled\": false }.\nAudit Method 2:\nIt is also possible to review the running configuration of a Kubelet via the /configz endpoint of the Kubernetes API. This can be achieved using kubectl to proxy your requests to the API.\nDiscover all nodes in your cluster by running the following command:\nkubectl get nodes\n\nNext, initiate a proxy with kubectl on a local port of your choice. In this example we will use 8080:\nkubectl proxy --port=8080\n\nWith this running, in a separate terminal run the following command for each node:\nexport NODE_NAME=my-node-name\ncurl http://localhost:8080/api/v1/nodes/${NODE_NAME}/proxy/configz    \n\nThe curl command will return the API response which will be a JSON formatted string representing the Kubelet configuration.\nVerify that Anonymous Authentication is not enabled checking that \"authentication\": { \"anonymous\": { \"enabled\": false } is in the API response.",
    "remediation_procedure": "Remediation Method 1:\nIf configuring via the Kubelet config file, you first need to locate the file.\nTo do this, SSH to each node and execute the following command to find the kubelet process:\nps -ef | grep kubelet\n\nThe output of the above command provides details of the active kubelet process, from which we can see the location of the configuration file provided to the kubelet service with the --config argument. The file can be viewed with a command such as more or less, like so:\nsudo less /path/to/kubelet-config.json\n\nDisable Anonymous Authentication by setting the following parameter:\n\"authentication\": { \"anonymous\": { \"enabled\": false } }\n\nRemediation Method 2:\nIf using executable arguments, edit the kubelet service file on each worker node and ensure the below parameters are part of the KUBELET_ARGS variable string.\nFor systems using systemd, such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, then this file can be found at /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf. Otherwise, you may need to look up documentation for your chosen operating system to determine which service manager is configured:\n--anonymous-auth=false\n\nFor Both Remediation Steps:\nBased on your system, restart the kubelet service and check the service status.\nThe following example is for operating systems using systemd, such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, and invokes the systemctl command. If systemctl is not available then you will need to look up documentation for your chosen operating system to determine which service manager is configured:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l",
    "default_value": "See the GKE documentation for the default value."
  },
  {
    "control_id": "5.3",
    "control": "Disable Dormant Accounts",
    "IG1": "o",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889455,
    "recommendation_id": 4683007,
    "view_level": "4.1.4",
    "title": "Ensure that default service accounts are not actively used",
    "pivot_control_id": 405,
    "pivot_recommendation_id": 4683007,
    "url": "https://workbench.cisecurity.org/sections/2889455/recommendations/4683007",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "The default service account should not be used to ensure that rights granted to applications can be more easily audited and reviewed.",
    "rationale_statement": "Kubernetes provides a default service account which is used by cluster workloads where no specific service account is assigned to the pod.\nWhere access to the Kubernetes API from a pod is required, a specific service account should be created for that pod, and rights granted to that service account.\nThe default service account should be configured such that it does not provide a service account token and does not have any explicit rights assignments.",
    "impact_statement": "All workloads which require access to the Kubernetes API will require an explicit service account to be created.",
    "audit_procedure": "For each namespace in the cluster, review the rights assigned to the default service account and ensure that it has no roles or cluster roles bound to it apart from the defaults.\nAdditionally ensure that the automountServiceAccountToken: false setting is in place for each default service account.",
    "remediation_procedure": "Create explicit service accounts wherever a Kubernetes workload requires specific access to the Kubernetes API server.\nModify the configuration of each default service account to include this value\nautomountServiceAccountToken: false",
    "default_value": "By default the default service account allows for its service account token to be mounted in pods in its namespace."
  },
  {
    "control_id": "5.4",
    "control": "Restrict Administrator Privileges to Dedicated Administrator Accounts",
    "IG1": "o",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889456,
    "recommendation_id": 4682996,
    "view_level": "3.2.2",
    "title": "Ensure that the --authorization-mode argument is not set to AlwaysAllow",
    "pivot_control_id": 406,
    "pivot_recommendation_id": 4682996,
    "url": "https://workbench.cisecurity.org/sections/2889456/recommendations/4682996",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Do not allow all requests. Enable explicit authorization.",
    "rationale_statement": "Kubelets can be configured to allow all authenticated requests (even anonymous ones) without needing explicit authorization checks from the apiserver. You should restrict this behavior and only allow explicitly authorized requests.",
    "impact_statement": "Unauthorized requests will be denied.",
    "audit_procedure": "Audit Method 1:\nKubelets can accept configuration via a configuration file and in some cases via command line arguments. It is important to note that parameters provided as command line arguments will override their counterpart parameters in the configuration file (see --config details in the Kubelet CLI Reference for more info, where you can also find out which configuration parameters can be supplied as a command line argument).\nWith this in mind, it is important to check for the existence of command line arguments as well as configuration file entries when auditing Kubelet configuration.\nFirstly, SSH to each node and execute the following command to find the Kubelet process:\nps -ef | grep kubelet\n\nThe output of the above command provides details of the active Kubelet process, from which we can see the command line arguments provided to the process. Also note the location of the configuration file, provided with the --config argument, as this will be needed to verify configuration. The file can be viewed with a command such as more or less, like so:\nsudo less /path/to/kubelet-config.json\n\nVerify that Webhook Authentication is enabled. This may be enabled as a command line argument to the kubelet service with --authentication-token-webhook or in the kubelet configuration file via \"authentication\": { \"webhook\": { \"enabled\": true } }.\nVerify that the Authorization Mode is set to WebHook. This may be set as a command line argument to the kubelet service with --authorization-mode=Webhook or in the configuration file via \"authorization\": { \"mode\": \"Webhook }.\nAudit Method 2:\nIt is also possible to review the running configuration of a Kubelet via the /configz endpoint of the Kubernetes API. This can be achieved using kubectl to proxy your requests to the API.\nDiscover all nodes in your cluster by running the following command:\nkubectl get nodes\n\nNext, initiate a proxy with kubectl on a local port of your choice. In this example we will use 8080:\nkubectl proxy --port=8080\n\nWith this running, in a separate terminal run the following command for each node:\nexport NODE_NAME=my-node-name\ncurl http://localhost:8080/api/v1/nodes/${NODE_NAME}/proxy/configz    \n\nThe curl command will return the API response which will be a JSON formatted string representing the Kubelet configuration.\nVerify that Webhook Authentication is enabled with \"authentication\": { \"webhook\": { \"enabled\": true } } in the API response.\nVerify that the Authorization Mode is set to WebHook with \"authorization\": { \"mode\": \"Webhook } in the API response.",
    "remediation_procedure": "Remediation Method 1:\nIf configuring via the Kubelet config file, you first need to locate the file.\nTo do this, SSH to each node and execute the following command to find the kubelet process:\nps -ef | grep kubelet\n\nThe output of the above command provides details of the active kubelet process, from which we can see the location of the configuration file provided to the kubelet service with the --config argument. The file can be viewed with a command such as more or less, like so:\nsudo less /path/to/kubelet-config.json\n\nEnable Webhook Authentication by setting the following parameter:\n\"authentication\": { \"webhook\": { \"enabled\": true } }\n\nNext, set the Authorization Mode to Webhook by setting the following parameter:\n\"authorization\": { \"mode\": \"Webhook }\n\nFiner detail of the authentication and authorization fields can be found in the Kubelet Configuration documentation.\nRemediation Method 2:\nIf using executable arguments, edit the kubelet service file on each worker node and ensure the below parameters are part of the KUBELET_ARGS variable string.\nFor systems using systemd, such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, then this file can be found at /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf. Otherwise, you may need to look up documentation for your chosen operating system to determine which service manager is configured:\n--authentication-token-webhook\n--authorization-mode=Webhook\n\nFor Both Remediation Steps:\nBased on your system, restart the kubelet service and check the service status.\nThe following example is for operating systems using systemd, such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, and invokes the systemctl command. If systemctl is not available then you will need to look up documentation for your chosen operating system to determine which service manager is configured:\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l",
    "default_value": "See the GKE documentation for the default value."
  },
  {
    "control_id": "5.4",
    "control": "Restrict Administrator Privileges to Dedicated Administrator Accounts",
    "IG1": "o",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889455,
    "recommendation_id": 4682993,
    "view_level": "4.1.1",
    "title": "Ensure that the cluster-admin role is only used where required",
    "pivot_control_id": 406,
    "pivot_recommendation_id": 4682993,
    "url": "https://workbench.cisecurity.org/sections/2889455/recommendations/4682993",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "The RBAC role cluster-admin provides wide-ranging powers over the environment and should be used only where and when needed.",
    "rationale_statement": "Kubernetes provides a set of default roles where RBAC is used. Some of these roles such as cluster-admin provide wide-ranging privileges which should only be applied where absolutely necessary. Roles such as cluster-admin allow super-user access to perform any action on any resource. When used in a ClusterRoleBinding, it gives full control over every resource in the cluster and in all namespaces. When used in a RoleBinding, it gives full control over every resource in the rolebinding's namespace, including the namespace itself.",
    "impact_statement": "Care should be taken before removing any clusterrolebindings from the environment to ensure they were not required for operation of the cluster. Specifically, modifications should not be made to clusterrolebindings with the system: prefix as they are required for the operation of system components.",
    "audit_procedure": "Obtain a list of the principals who have access to the cluster-admin role by reviewing the clusterrolebinding output for each role binding that has access to the cluster-admin role.\nkubectl get clusterrolebindings -o=custom-columns=NAME:.metadata.name,ROLE:.roleRef.name,SUBJECT:.subjects[*].name\n\nReview each principal listed and ensure that cluster-admin privilege is required for it.",
    "remediation_procedure": "Identify all clusterrolebindings to the cluster-admin role. Check if they are used and if they need this role or if they could use a role with fewer privileges.\nWhere possible, first bind users to a lower-privileged role and then remove the clusterrolebinding to the cluster-admin role :\nkubectl delete clusterrolebinding [name]",
    "default_value": "By default a single clusterrolebinding called cluster-admin is provided with the system:masters group as its principal."
  },
  {
    "control_id": "5.4",
    "control": "Restrict Administrator Privileges to Dedicated Administrator Accounts",
    "IG1": "o",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889455,
    "recommendation_id": 4683019,
    "view_level": "4.1.6",
    "title": "Avoid use of system:masters group",
    "pivot_control_id": 406,
    "pivot_recommendation_id": 4683019,
    "url": "https://workbench.cisecurity.org/sections/2889455/recommendations/4683019",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "The special group system:masters should not be used to grant permissions to any user or service account, except where strictly necessary (e.g. bootstrapping access prior to RBAC being fully available)",
    "rationale_statement": "The system:masters group has unrestricted access to the Kubernetes API hard-coded into the API server source code. An authenticated user who is a member of this group cannot have their access reduced, even if all bindings and cluster role bindings which mention it, are removed.\nWhen combined with client certificate authentication, use of this group can allow for irrevocable cluster-admin level credentials to exist for a cluster.\nGKE includes the CertificateSubjectRestriction admission controller which rejects requests for the system:masters group.\nCertificateSubjectRestriction \"This admission controller observes creation of CertificateSigningRequest resources that have a spec.signerName of kubernetes.io/kube-apiserver-client. It rejects any request that specifies a 'group' (or 'organization attribute') of system:masters.\" https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#certificatesubjectrestriction",
    "impact_statement": "Once the RBAC system is operational in a cluster system:masters should not be specifically required, as ordinary bindings from principals to the cluster-admin cluster role can be made where unrestricted access is required.",
    "audit_procedure": "Review a list of all credentials which have access to the cluster and ensure that the group system:masters is not used.",
    "remediation_procedure": "Remove the system:masters group from all users in the cluster.",
    "default_value": "By default some clusters will create a \"break glass\" client certificate which is a member of this group. Access to this client certificate should be carefully controlled and it should not be used for general cluster operations."
  },
  {
    "control_id": "5.4",
    "control": "Restrict Administrator Privileges to Dedicated Administrator Accounts",
    "IG1": "o",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889455,
    "recommendation_id": 4683021,
    "view_level": "4.1.7",
    "title": "Limit use of the Bind, Impersonate and Escalate permissions in the Kubernetes cluster",
    "pivot_control_id": 406,
    "pivot_recommendation_id": 4683021,
    "url": "https://workbench.cisecurity.org/sections/2889455/recommendations/4683021",
    "assessment_status": "Manual",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Cluster roles and roles with the impersonate, bind or escalate permissions should not be granted unless strictly required. Each of these permissions allow a particular subject to escalate their privileges beyond those explicitly granted by cluster administrators",
    "rationale_statement": "The impersonate privilege allows a subject to impersonate other users gaining their rights to the cluster. The bind privilege allows the subject to add a binding to a cluster role or role which escalates their effective permissions in the cluster. The escalate privilege allows a subject to modify cluster roles to which they are bound, increasing their rights to that level.\nEach of these permissions has the potential to allow for privilege escalation to cluster-admin level.",
    "impact_statement": "There are some cases where these permissions are required for cluster service operation, and care should be taken before removing these permissions from system service accounts.",
    "audit_procedure": "Review the users who have access to cluster roles or roles which provide the impersonate, bind or escalate privileges.",
    "remediation_procedure": "Where possible, remove the impersonate, bind and escalate rights from subjects.",
    "default_value": "In a default kubeadm cluster, the system:masters group and clusterrole-aggregation-controller service account have access to the escalate privilege. The system:masters group also has access to bind and impersonate."
  },
  {
    "control_id": "5.5",
    "control": "Establish and Maintain an Inventory of Service Accounts",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889455,
    "recommendation_id": 4683022,
    "view_level": "4.1.8",
    "title": "Avoid bindings to system:anonymous",
    "pivot_control_id": 407,
    "pivot_recommendation_id": 4683022,
    "url": "https://workbench.cisecurity.org/sections/2889455/recommendations/4683022",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Avoid ClusterRoleBindings nor RoleBindings with the user system:anonymous.",
    "rationale_statement": "Kubernetes assigns user system:anonymous to API server requests that have no authentication information provided. Binding a role to user system:anonymous gives any unauthenticated user the permissions granted by that role and is strongly discouraged.",
    "impact_statement": "Unauthenticated users will have privileges and permissions associated with roles associated with the configured bindings.\nCare should be taken before removing any clusterrolebindings or rolebindings from the environment to ensure they were not required for operation of the cluster. Use a more specific and authenticated user for cluster operations.",
    "audit_procedure": "Both CusterRoleBindings and RoleBindings should be audited. Use the following command to confirm there are no ClusterRoleBindings to system:anonymous:\n$ kubectl get clusterrolebindings -o json   | jq -r '[\"Name\"], [\"-----\"], (.items[] | select((.subjects | length) > 0) | select(any(.subjects[]; .name == \"system:anonymous\")) | [.metadata.namespace, .metadata.name]) | @tsv'\n\nThere should be no ClusterRoleBindings listed. If any bindings exist, review their permissions with the following command and reassess their privilege.\n$ kubectl get clusterrolebinding [CLUSTER_ROLE_BINDING_NAME] -o json \\\n    | jq ' .roleRef.name +\" \" + .roleRef.kind' \\\n    | sed -e 's/\"//g' \\\n    | xargs -l bash -c 'kubectl get $1 $0 -o yaml'\n\n\nConfirm that there are no RoleBindings including the system:anonymous user:\n$ kubectl get rolebindings -A -o json \\\n  | jq -r '[\"Namespace\", \"Name\"], [\"---------\", \"-----\"], (.items[] | select((.subjects | length) > 0) | select(any(.subjects[]; .name == \"system:anonymous\")) | [.metadata.namespace, .metadata.name]) | @tsv'\n\n\nThere should be no RoleBindings listed.\nIf any bindings exist, review their permissions with the following command and reassess their privilege.\n$ kubectl get rolebinding [ROLE_BINDING_NAME] --namespace [ROLE_BINDING_NAMESPACE] -o json \\\n    | jq ' .roleRef.name +\" \" + .roleRef.kind' \\\n    | sed -e 's/\"//g' \\\n    | xargs -l bash -c 'kubectl get $1 $0 -o yaml --namespace [ROLE_BINDING_NAMESPACE]'",
    "remediation_procedure": "Identify all clusterrolebindings and rolebindings to the user system:anonymous. Check if they are used and review the permissions associated with the binding using the commands in the Audit section above or refer to GKE documentation.\nStrongly consider replacing unsafe bindings with an authenticated, user-defined group. Where possible, bind to non-default, user-defined groups with least-privilege roles.\nIf there are any unsafe bindings to the user system:anonymous, proceed to delete them after consideration for cluster operations with only necessary, safer bindings.\nkubectl delete clusterrolebinding\n[CLUSTER_ROLE_BINDING_NAME]\n\nkubectl delete rolebinding\n[ROLE_BINDING_NAME]\n--namespace\n[ROLE_BINDING_NAMESPACE]",
    "default_value": "No clusterrolebindings nor rolebindings with user system:anonymous."
  },
  {
    "control_id": "5.5",
    "control": "Establish and Maintain an Inventory of Service Accounts",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889455,
    "recommendation_id": 4683025,
    "view_level": "4.1.9",
    "title": "Avoid non-default bindings to system:unauthenticated",
    "pivot_control_id": 407,
    "pivot_recommendation_id": 4683025,
    "url": "https://workbench.cisecurity.org/sections/2889455/recommendations/4683025",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Avoid non-default ClusterRoleBindings and RoleBindings with the group system:unauthenticated, except the ClusterRoleBinding system:public-info-viewer.",
    "rationale_statement": "Kubernetes assigns the group system:unauthenticated to API server requests that have no authentication information provided. Binding a role to this group gives any unauthenticated user the permissions granted by that role and is strongly discouraged.",
    "impact_statement": "Unauthenticated users will have privileges and permissions associated with roles associated with the configured bindings.\nCare should be taken before removing any non-default clusterrolebindings or rolebindings from the environment to ensure they were not required for operation of the cluster. Leverage a more specific and authenticated user for cluster operations.",
    "audit_procedure": "Both CusterRoleBindings and RoleBindings should be audited. Use the following command to confirm there are no non-default ClusterRoleBindings to group system:unauthenticated:\n$ kubectl get clusterrolebindings -o json   | jq -r '[\"Name\"], [\"-----\"], (.items[] | select((.subjects | length) > 0) | select(any(.subjects[]; .name == \"system:unauthenticated\")) | [.metadata.namespace, .metadata.name]) | @tsv'\n\nOnly the following default ClusterRoleBinding should be displayed:\nName\n-----\n        system:public-info-viewer\n\nIf any non-default bindings exist, review their permissions with the following command and reassess their privilege.\n$ kubectl get clusterrolebinding [CLUSTER_ROLE_BINDING_NAME] -o json \\\n    | jq ' .roleRef.name +\" \" + .roleRef.kind' \\\n    | sed -e 's/\"//g' \\\n    | xargs -l bash -c 'kubectl get $1 $0 -o yaml'\n\nConfirm that there are no RoleBindings including the system:unauthenticated group:\n$ kubectl get rolebindings -A -o json \\\n  | jq -r '[\"Namespace\", \"Name\"], [\"---------\", \"-----\"], (.items[] | select((.subjects | length) > 0) | select(any(.subjects[]; .name == \"system:unauthenticated\")) | [.metadata.namespace, .metadata.name]) | @tsv'\n\nThere should be no RoleBindings listed.\nIf any bindings exist, review their permissions with the following command and reassess their privilege.\n$ kubectl get rolebinding [ROLE_BINDING_NAME] --namespace [ROLE_BINDING_NAMESPACE] -o json \\\n    | jq ' .roleRef.name +\" \" + .roleRef.kind' \\\n    | sed -e 's/\"//g' \\\n    | xargs -l bash -c 'kubectl get $1 $0 -o yaml --namespace [ROLE_BINDING_NAMESPACE]'",
    "remediation_procedure": "Identify all non-default clusterrolebindings and rolebindings to the group system:unauthenticated. Check if they are used and review the permissions associated with the binding using the commands in the Audit section above or refer to GKE documentation.\nStrongly consider replacing non-default, unsafe bindings with an authenticated, user-defined group. Where possible, bind to non-default, user-defined groups with least-privilege roles.\nIf there are any non-default, unsafe bindings to the group system:unauthenticated, proceed to delete them after consideration for cluster operations with only necessary, safer bindings.\nkubectl delete clusterrolebinding\n[CLUSTER_ROLE_BINDING_NAME]\n\nkubectl delete rolebinding\n[ROLE_BINDING_NAME]\n--\nnamespace\n[ROLE_BINDING_NAMESPACE]",
    "default_value": "ClusterRoleBindings with group system:unauthenticated:\n\nsystem:public-info-viewer\n\nNo RoleBindings with the group system:unauthenticated."
  },
  {
    "control_id": "5.5",
    "control": "Establish and Maintain an Inventory of Service Accounts",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889455,
    "recommendation_id": 4683028,
    "view_level": "4.1.10",
    "title": "Avoid non-default bindings to system:authenticated",
    "pivot_control_id": 407,
    "pivot_recommendation_id": 4683028,
    "url": "https://workbench.cisecurity.org/sections/2889455/recommendations/4683028",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Avoid non-default ClusterRoleBindings and RoleBindings with the group system:authenticated, except the ClusterRoleBindings system:basic-user, system:discovery, and system:public-info-viewer.\nGoogle's approach to authentication is to make authenticating to Google Cloud and GKE as simple and secure as possible without adding complex configuration steps. The group system:authenticated includes all users with a Google account, which includes all Gmail accounts. Consider your authorization controls with this extended group scope when granting permissions. Thus, group system:authenticated is not recommended for non-default use.",
    "rationale_statement": "GKE assigns the group system:authenticated to API server requests made by any user who is signed in with a Google Account, including all Gmail accounts. In practice, this isn't meaningfully different from system:unauthenticated because anyone can create a Google Account.\nBinding a role to the group system:authenticated gives any user with a Google Account, including all Gmail accounts, the permissions granted by that role and is strongly discouraged.",
    "impact_statement": "Authenticated users in group system:authenticated should be treated similarly to users in system:unauthenticated, having privileges and permissions associated with roles associated with the configured bindings.\nCare should be taken before removing any non-default clusterrolebindings or rolebindings from the environment to ensure they were not required for operation of the cluster. Leverage a more specific and authenticated user for cluster operations.",
    "audit_procedure": "Use the following command to confirm there are no non-default ClusterRoleBindings to system:authenticated:\n$ kubectl get clusterrolebindings -o json   | jq -r '[\"Name\"], [\"-----\"], (.items[] | select((.subjects | length) > 0) | select(any(.subjects[]; .name == \"system:unauthenticated\")) | [.metadata.namespace, .metadata.name]) | @tsv'\n\nOnly the following default ClusterRoleBindings should be displayed:\nName\n-----\n        system:basic-user\n        system:discovery\n        system:public-info-viewer\n\nIf any non-default bindings exist, review their permissions with the following command and reassess their privilege.\n$ kubectl get clusterrolebinding CLUSTER_ROLE_BINDING_NAME -o json \\\n    | jq ' .roleRef.name +\" \" + .roleRef.kind' \\\n    | sed -e 's/\"//g' \\\n    | xargs -l bash -c 'kubectl get $1 $0 -o yaml'\n\nConfirm that there are no RoleBindings including the system:authenticated group:\n$ kubectl get rolebindings -A -o json \\\n  | jq -r '[\"Namespace\", \"Name\"], [\"---------\", \"-----\"], (.items[] | select((.subjects | length) > 0) | select(any(.subjects[]; .name == \"system:unauthenticated\")) | [.metadata.namespace, .metadata.name]) | @tsv'\n\nThere should be no RoleBindings listed.\nIf any bindings exist, review their permissions with the following command and reassess their privilege.\n$ kubectl get rolebinding [ROLE_BINDING_NAME] --namespace [ROLE_BINDING_NAMESPACE] -o json \\\n    | jq ' .roleRef.name +\" \" + .roleRef.kind' \\\n    | sed -e 's/\"//g' \\\n    | xargs -l bash -c 'kubectl get $1 $0 -o yaml --namespace [ROLE_BINDING_NAMESPACE]'",
    "remediation_procedure": "Identify all non-default clusterrolebindings and rolebindings to the group system:authenticated. Check if they are used and review the permissions associated with the binding using the commands in the Audit section above or refer to GKE documentation.\nStrongly consider replacing non-default, unsafe bindings with an authenticated, user-defined group. Where possible, bind to non-default, user-defined groups with least-privilege roles.\nIf there are any non-default, unsafe bindings to the group system:authenticated, proceed to delete them after consideration for cluster operations with only necessary, safer bindings.\nkubectl delete clusterrolebinding\n[CLUSTER_ROLE_BINDING_NAME]\n\nkubectl delete rolebinding\n[ROLE_BINDING_NAME]\n--namespace\n[ROLE_BINDING_NAMESPACE]",
    "default_value": "ClusterRoleBindings with group system:authenticated:\n\nsystem:basic-user\nsystem:discovery\n\nNo RoleBindings with the group system:authenticated."
  },
  {
    "control_id": "6.8",
    "control": "Define and Maintain Role-Based Access Control",
    "IG1": "-",
    "IG2": "-",
    "IG3": "o",
    "section_id": 2889468,
    "recommendation_id": 4683042,
    "view_level": "5.8.1",
    "title": "Ensure authentication using Client Certificates is Disabled",
    "pivot_control_id": 417,
    "pivot_recommendation_id": 4683042,
    "url": "https://workbench.cisecurity.org/sections/2889468/recommendations/4683042",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Disable Client Certificates, which require certificate rotation, for authentication. Instead, use another authentication method like OpenID Connect.",
    "rationale_statement": "With Client Certificate authentication, a client presents a certificate that the API server verifies with the specified Certificate Authority. In GKE, Client Certificates are signed by the cluster root Certificate Authority. When retrieved, the Client Certificate is only base64 encoded and not encrypted.\nGKE manages authentication via gcloud for you using the OpenID Connect token method, setting up the Kubernetes configuration, getting an access token, and keeping it up to date. This means Basic Authentication using static passwords and Client Certificate authentication, which both require additional management overhead of key management and rotation, are not necessary and should be disabled.\nWhen Client Certificate authentication is disabled, you will still be able to authenticate to the cluster with other authentication methods, such as OpenID Connect tokens. See also Recommendation 6.8.1 to disable authentication using static passwords, known as Basic Authentication.",
    "impact_statement": "Users will no longer be able to authenticate with the pre-provisioned x509 certificate. You will have to configure and use alternate authentication mechanisms, such as OpenID Connect tokens.",
    "audit_procedure": "The audit script for this recommendation utilizes 3 variables:\n$CLUSTER_NAME\n$COMPUTE_ZONE\nPlease set these parameters on the system where you will be executing your gcloud audit script or command.\nUsing Google Cloud Console\n\nGo to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\nFrom the list of clusters, click on the desired cluster. On the Details pane, make sure 'Client certificate' is set to 'Disabled'.\n\nUsing Command line\nTo check that the client certificate has not been issued, run the following command:\ngcloud container clusters describe $CLUSTER_NAME \\\n  --zone $COMPUTE_ZONE \\\n  --format json | jq '.masterAuth.clientKey'\n\nThe output of the above command returns null ({ }) if the client certificate has not been issued for the cluster (Client Certificate authentication is disabled).\nNote.  Depreciated as of v1.19. For Basic Authentication, Legacy authorization can be edited for standard clusters but cannot be edited in Autopilot clusters.",
    "remediation_procedure": "Currently, there is no way to remove a client certificate from an existing cluster. Thus a new cluster must be created.\nUsing Google Cloud Console\n\nGo to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\nClick CREATE CLUSTER\nConfigure as required and the click on 'Availability, networking, security, and additional features' section\nEnsure that the 'Issue a client certificate' checkbox is not ticked\nClick CREATE.\n\nUsing Command Line\nCreate a new cluster without a Client Certificate:\ngcloud container clusters create [CLUSTER_NAME] \\ \n --no-issue-client-certificate",
    "default_value": "Google Kubernetes Engine (GKE), both Basic Authentication and Client Certificate issuance are disabled by default for new clusters. This change was implemented starting with GKE version 1.12 to enhance security by reducing the attack surface associated with these authentication methods."
  },
  {
    "control_id": "6.8",
    "control": "Define and Maintain Role-Based Access Control",
    "IG1": "-",
    "IG2": "-",
    "IG3": "o",
    "section_id": 2889468,
    "recommendation_id": 4683044,
    "view_level": "5.8.2",
    "title": "Manage Kubernetes RBAC users with Google Groups for GKE",
    "pivot_control_id": 417,
    "pivot_recommendation_id": 4683044,
    "url": "https://workbench.cisecurity.org/sections/2889468/recommendations/4683044",
    "assessment_status": "Manual",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Cluster Administrators should leverage G Suite Groups and Cloud IAM to assign Kubernetes user roles to a collection of users, instead of to individual emails using only Cloud IAM.",
    "rationale_statement": "On- and off-boarding users is often difficult to automate and prone to error. Using a single source of truth for user permissions via G Suite Groups reduces the number of locations that an individual must be off-boarded from, and prevents users gaining unique permissions sets that increase the cost of audit.",
    "impact_statement": "When migrating to using security groups, an audit of RoleBindings and ClusterRoleBindings is required to ensure all users of the cluster are managed using the new groups and not individually.\nWhen managing RoleBindings and ClusterRoleBindings, be wary of inadvertently removing bindings required by service accounts.",
    "audit_procedure": "Using G Suite Admin Console and Google Cloud Console\n\nNavigate to manage G Suite Groups in the Google Admin console at: https://admin.google.com/dashboard\nEnsure there is a group named gke-security-groups@[yourdomain.com]. The group must be named exactly gke-security-groups.\nEnsure only further groups (not individual users) are included in the gke-security-groups group as members.\nGo to the Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\nFrom the list of clusters, click on the desired cluster. In the Details pane, make sure Google Groups for RBAC is set to Enabled.",
    "remediation_procedure": "Follow the G Suite Groups instructions at: https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control#google-groups-for-gke.\nThen, create a cluster with:\ngcloud container clusters create <cluster_name> --security-group <security_group_name>\n\nFinally create Roles, ClusterRoles, RoleBindings, and ClusterRoleBindings that reference the G Suite Groups.",
    "default_value": "Google Groups for GKE is disabled by default."
  },
  {
    "control_id": "6.8",
    "control": "Define and Maintain Role-Based Access Control",
    "IG1": "-",
    "IG2": "-",
    "IG3": "o",
    "section_id": 2889468,
    "recommendation_id": 4683045,
    "view_level": "5.8.3",
    "title": "Ensure Legacy Authorization (ABAC) is Disabled",
    "pivot_control_id": 417,
    "pivot_recommendation_id": 4683045,
    "url": "https://workbench.cisecurity.org/sections/2889468/recommendations/4683045",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Legacy Authorization, also known as Attribute-Based Access Control (ABAC) has been superseded by Role-Based Access Control (RBAC) and is not under active development.\nRBAC is the recommended way to manage permissions in Kubernetes.",
    "rationale_statement": "In Kubernetes, RBAC is used to grant permissions to resources at the cluster and namespace level. RBAC allows the definition of roles with rules containing a set of permissions, whilst the legacy authorizer (ABAC) in Kubernetes Engine grants broad, statically defined permissions. As RBAC provides significant security advantages over ABAC, it is recommended option for access control. Where possible, legacy authorization must be disabled for GKE clusters.",
    "impact_statement": "Once the cluster has the legacy authorizer disabled, the user must be granted the ability to create authorization roles using RBAC to ensure that the role-based access control permissions take effect.",
    "audit_procedure": "Using Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\nFrom the list of clusters, click on each cluster to open the Details pane, and make sure 'Legacy Authorization' is set to 'Disabled'.\n\nUsing Command Line:\nTo check Legacy Authorization status for an existing cluster, run the following command:\ngcloud container clusters describe <cluster_name> --zone <compute_zone> --format json | jq '.legacyAbac'\n\nThe output should return null ({}) if Legacy Authorization is Disabled. If Legacy Authorization is Enabled, the above command will return true value.",
    "remediation_procedure": "Using Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\nSelect Kubernetes clusters for which Legacy Authorization is enabled.\nClick EDIT.\nSet 'Legacy Authorization' to 'Disabled'.\nClick SAVE.\n\nUsing Command Line:\nTo disable Legacy Authorization for an existing cluster, run the following command:\ngcloud container clusters update <cluster_name> --zone <compute_zone> --no-enable-legacy-authorization",
    "default_value": "Kubernetes Engine clusters running GKE version 1.8 and later disable the legacy authorization system by default, and thus role-based access control permissions take effect with no special action required."
  },
  {
    "control_id": "7.3",
    "control": "Perform Automated Operating System Patch Management",
    "IG1": "o",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889462,
    "recommendation_id": 4683018,
    "view_level": "5.5.3",
    "title": "Ensure Node Auto-Upgrade is enabled for GKE nodes",
    "pivot_control_id": 421,
    "pivot_recommendation_id": 4683018,
    "url": "https://workbench.cisecurity.org/sections/2889462/recommendations/4683018",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Node auto-upgrade keeps nodes at the current Kubernetes and OS security patch level to mitigate known vulnerabilities.",
    "rationale_statement": "Node auto-upgrade helps you keep the nodes in the cluster or node pool up to date with the latest stable patch version of Kubernetes as well as the underlying node operating system. Node auto-upgrade uses the same update mechanism as manual node upgrades.\nNode pools with node auto-upgrade enabled are automatically scheduled for upgrades when a new stable Kubernetes version becomes available. When the upgrade is performed, the Node pool is upgraded to match the current cluster master version. From a security perspective, this has the benefit of applying security updates automatically to the Kubernetes Engine when security fixes are released.",
    "impact_statement": "Enabling node auto-upgrade does not cause the nodes to upgrade immediately. Automatic upgrades occur at regular intervals at the discretion of the Kubernetes Engine team.\nTo prevent upgrades occurring during a peak period for the cluster, a maintenance window should be defined. A maintenance window is a four-hour timeframe that can be chosen, during which automatic upgrades should occur. Upgrades can occur on any day of the week, and at any time within the timeframe. To prevent upgrades from occurring during certain dates, a maintenance exclusion should be defined. A maintenance exclusion can span multiple days.",
    "audit_procedure": "Using Google Cloud Console\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list\nFrom the list of clusters, select the desired cluster. For each Node pool, view the Node pool Details pane and ensure that under the 'Management' heading, 'Auto-upgrade' is set to 'Enabled'.\n\nUsing Command Line\nTo check the existence of node auto-upgrade for an existing cluster's Node pool, run:\ngcloud container node-pools describe <node_pool_name> --cluster <cluster_name> --zone <cluster_zone> --format json | jq '.management'\n\nEnsure the output of the above command has JSON key attribute autoUpgrade set to true:\n{\n  \"autoUpgrade\": true\n}\n\nIf node auto-upgrade is disabled, the output of the above command output will not contain the autoUpgrade entry.",
    "remediation_procedure": "Using Google Cloud Console\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\nSelect the Kubernetes cluster containing the node pool for which auto-upgrade disabled.\nSelect the Node pool by clicking on the name of the pool.\nNavigate to the Node pool details pane and click EDIT.\nUnder the Management heading, check the Enable auto-repair box.\nClick SAVE.\nRepeat steps 2-6 for every cluster and node pool with auto-upgrade disabled.\n\nUsing Command Line\nTo enable node auto-upgrade for an existing cluster's Node pool, run the following command:\ngcloud container node-pools update <node_pool_name> --cluster <cluster_name> --zone <cluster_zone> --enable-autoupgrade",
    "default_value": "Node auto-upgrade is enabled by default.\nEven if a cluster has been created with node auto-repair enabled, this only applies to the default Node pool. Subsequent node pools do not have node auto-upgrade enabled by default."
  },
  {
    "control_id": "7.4",
    "control": "Perform Automated Application Patch Management",
    "IG1": "o",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889462,
    "recommendation_id": 4683020,
    "view_level": "5.5.4",
    "title": "When creating New Clusters - Automate GKE version management using Release Channels",
    "pivot_control_id": 422,
    "pivot_recommendation_id": 4683020,
    "url": "https://workbench.cisecurity.org/sections/2889462/recommendations/4683020",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Subscribe to the Regular or Stable Release Channel to automate version upgrades to the GKE cluster and to reduce version management complexity to the number of features and level of stability required.",
    "rationale_statement": "Release Channels signal a graduating level of stability and production-readiness. These are based on observed performance of GKE clusters running that version and represent experience and confidence in the cluster version.\nThe Regular release channel upgrades every few weeks and is for production users who need features not yet offered in the Stable channel. These versions have passed internal validation, but don't have enough historical data to guarantee their stability. Known issues generally have known workarounds.\nThe Stable release channel upgrades every few months and is for production users who need stability above all else, and for whom frequent upgrades are too risky. These versions have passed internal validation and have been shown to be stable and reliable in production, based on the observed performance of those clusters.\nCritical security patches are delivered to all release channels.",
    "impact_statement": "Once release channels are enabled on a cluster, they cannot be disabled. To stop using release channels, the cluster must be recreated without the --release-channel flag.\nNode auto-upgrade is enabled (and cannot be disabled), so the cluster is updated automatically from releases available in the chosen release channel.",
    "audit_procedure": "Using Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\nFrom the list of clusters, select the desired cluster.\nWithin the Details pane, if using a release channel, the release channel should be set to the Regular or Stable channel.\n\nUsing Command Line:\nRun the following command:\ngcloud container clusters describe $CLUSTER_NAME --zone $COMPUTE_ZONE --format json | jq .releaseChannel.channel\n\nReturned Value:\n\n\"REGULAR\"\n\n\nThe output of the above command will return regular or stable if these release channels are being used to manage automatic upgrades for the cluster.",
    "remediation_procedure": "Currently, cluster Release Channels are only configurable at cluster provisioning time.\nUsing Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\nClick CREATE, and choose CONFIGURE for the required cluster mode.\nUnder the Control plane version heading, click the Release Channels button.\nSelect the Regular or Stable channels from the Release Channel drop-down menu.\nConfigure the rest of the cluster settings as required.\nClick CREATE.\n\nUsing Command Line:\nCreate a new cluster by running the following command:\ngcloud container clusters create <cluster_name> --zone <cluster_zone> --release-channel <release_channel>\n\nwhere <release_channel> is stable or regular, according to requirements.",
    "default_value": "Currently, release channels are not enabled by default."
  },
  {
    "control_id": "7.5",
    "control": "Perform Automated Vulnerability Scans of Internal Enterprise Assets",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889462,
    "recommendation_id": 4683024,
    "view_level": "5.5.6",
    "title": "Ensure Integrity Monitoring for Shielded GKE Nodes is Enabled",
    "pivot_control_id": 423,
    "pivot_recommendation_id": 4683024,
    "url": "https://workbench.cisecurity.org/sections/2889462/recommendations/4683024",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Enable Integrity Monitoring for Shielded GKE Nodes to be notified of inconsistencies during the node boot sequence.",
    "rationale_statement": "Integrity Monitoring provides active alerting for Shielded GKE nodes which allows administrators to respond to integrity failures and prevent compromised nodes from being deployed into the cluster.",
    "impact_statement": "None.",
    "audit_procedure": "Using Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list\nFrom the list of clusters, click on the name of the cluster under test.\nOpen the Details pane for each Node pool within the cluster, and ensure that 'Integrity monitoring' is set to 'Enabled' under the Security heading.\n\nUsing Command Line:\nTo check if Integrity Monitoring is enabled for the Node pools in the cluster, run the following command for each Node pool:\ngcloud container node-pools describe <node_pool_name> --cluster <cluster_name> --zone <compute_zone> --format json | jq .config.shieldedInstanceConfig\n\nThis will return the following, if Integrity Monitoring is enabled:\n{\n   \"enableIntegrityMonitoring\": true\n}",
    "remediation_procedure": "Once a Node pool is provisioned, it cannot be updated to enable Integrity Monitoring. New Node pools must be created within the cluster with Integrity Monitoring enabled.\nUsing Google Cloud Console\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list\nFrom the list of clusters, click on the cluster requiring the update and click ADD NODE POOL.\nEnsure that the 'Integrity monitoring' checkbox is checked under the 'Shielded options' Heading.\nClick SAVE.\n\nWorkloads from existing non-conforming Node pools will need to be migrated to the newly created Node pool, then delete non-conforming Node pools to complete the remediation\nUsing Command Line\nTo create a Node pool within the cluster with Integrity Monitoring enabled, run the following command:\ngcloud container node-pools create <node_pool_name> --cluster <cluster_name> --zone <compute_zone> --shielded-integrity-monitoring\n\nWorkloads from existing non-conforming Node pools will need to be migrated to the newly created Node pool, then delete non-conforming Node pools to complete the remediation",
    "default_value": "Integrity Monitoring is disabled by default on GKE clusters. Integrity Monitoring is enabled by default for Shielded GKE Nodes; however, if Secure Boot is enabled at creation time, Integrity Monitoring is disabled."
  },
  {
    "control_id": "7.5",
    "control": "Perform Automated Vulnerability Scans of Internal Enterprise Assets",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889462,
    "recommendation_id": 4683026,
    "view_level": "5.5.7",
    "title": "Ensure Secure Boot for Shielded GKE Nodes is Enabled",
    "pivot_control_id": 423,
    "pivot_recommendation_id": 4683026,
    "url": "https://workbench.cisecurity.org/sections/2889462/recommendations/4683026",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Enable Secure Boot for Shielded GKE Nodes to verify the digital signature of node boot components.",
    "rationale_statement": "An attacker may seek to alter boot components to persist malware or root kits during system initialisation. Secure Boot helps ensure that the system only runs authentic software by verifying the digital signature of all boot components, and halting the boot process if signature verification fails.",
    "impact_statement": "Secure Boot will not permit the use of third-party unsigned kernel modules.",
    "audit_procedure": "Using Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\nFrom the list of clusters, click on the name of the cluster under test.\nOpen the Details pane for each Node pool within the cluster, and ensure that Secure boot is set to Enabled under the Security heading.\n\nUsing Command Line:\nTo check if Secure Boot is enabled for the Node pools in the cluster, run the following command for each Node pool:\ngcloud container node-pools describe <node_pool_name> --cluster <cluster_name> --zone <compute_zone> --format json | jq .config.shieldedInstanceConfig\n\nThis will return the value below, if Secure Boot is enabled:\n{\n  \"enableSecureBoot\": true\n}",
    "remediation_procedure": "Once a Node pool is provisioned, it cannot be updated to enable Secure Boot. New Node pools must be created within the cluster with Secure Boot enabled.\nUsing Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list\nFrom the list of clusters, click on the cluster requiring the update and click ADD NODE POOL.\nEnsure that the Secure boot checkbox is checked under the Shielded options Heading.\nClick SAVE.\n\nWorkloads will need to be migrated from existing non-conforming Node pools to the newly created Node pool, then delete the non-conforming pools.\nUsing Command Line:\nTo create a Node pool within the cluster with Secure Boot enabled, run the following command:\ngcloud container node-pools create <node_pool_name> --cluster <cluster_name> --zone <compute_zone> --shielded-secure-boot\n\nWorkloads will need to be migrated from existing non-conforming Node pools to the newly created Node pool, then delete the non-conforming pools.",
    "default_value": "By default, Secure Boot is disabled in GKE clusters. By default, Secure Boot is disabled when Shielded GKE Nodes is enabled."
  },
  {
    "control_id": "7.6",
    "control": "Perform Automated Vulnerability Scans of Externally-Exposed Enterprise Assets",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889458,
    "recommendation_id": 4683006,
    "view_level": "5.1.1",
    "title": "Ensure Image Vulnerability Scanning is enabled",
    "pivot_control_id": 424,
    "pivot_recommendation_id": 4683006,
    "url": "https://workbench.cisecurity.org/sections/2889458/recommendations/4683006",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Note: GCR is now deprecated, being superseded by Artifact Registry starting 15th May 2024. Runtime Vulnerability scanning is available via GKE Security Posture\nScan images stored in Google Container Registry (GCR) or Artifact Registry (AR) for vulnerabilities.",
    "rationale_statement": "Vulnerabilities in software packages can be exploited by malicious users to obtain unauthorized access to local cloud resources. GCR Container Analysis API or Artifact Registry Container Scanning API allow images stored in GCR or AR respectively to be scanned for known vulnerabilities.",
    "impact_statement": "None.",
    "audit_procedure": "For Images Hosted in GCR:\nUsing Google Cloud Console:\n\nGo to GCR by visiting https://console.cloud.google.com/gcr\nSelect Settings and check if Vulnerability scanning is Enabled.\n\nUsing Command Line:\ngcloud services list --enabled\n\nEnsure that the Container Registry API and Container Analysis API are listed in the output.\nFor Images Hosted in AR:\nUsing Google Cloud Console:\n\nGo to AR by visiting https://console.cloud.google.com/artifacts\nSelect Settings and check if Vulnerability scanning is Enabled.\n\nUsing Command Line:\ngcloud services list --enabled\n\nEnsure that Container Scanning API and Artifact Registry API are listed in the output.",
    "remediation_procedure": "For Images Hosted in GCR:\nUsing Google Cloud Console\n\nGo to GCR by visiting: https://console.cloud.google.com/gcr\nSelect Settings and, under the Vulnerability Scanning heading, click the TURN ON button.\n\nUsing Command Line\ngcloud services enable containeranalysis.googleapis.com\n\nFor Images Hosted in AR:\nUsing Google Cloud Console\n\nGo to GCR by visiting: https://console.cloud.google.com/artifacts\nSelect Settings and, under the Vulnerability Scanning heading, click the ENABLE button.\n\nUsing Command Line\ngcloud services enable containerscanning.googleapis.com",
    "default_value": "By default, GCR Container Analysis and AR Container Scanning are disabled."
  },
  {
    "control_id": "7.6",
    "control": "Perform Automated Vulnerability Scans of Externally-Exposed Enterprise Assets",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889462,
    "recommendation_id": 4683017,
    "view_level": "5.5.2",
    "title": "Ensure Node Auto-Repair is enabled for GKE nodes",
    "pivot_control_id": 424,
    "pivot_recommendation_id": 4683017,
    "url": "https://workbench.cisecurity.org/sections/2889462/recommendations/4683017",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Nodes in a degraded state are an unknown quantity and so may pose a security risk.",
    "rationale_statement": "Kubernetes Engine's node auto-repair feature helps you keep the nodes in the cluster in a healthy, running state. When enabled, Kubernetes Engine makes periodic checks on the health state of each node in the cluster. If a node fails consecutive health checks over an extended time period, Kubernetes Engine initiates a repair process for that node.",
    "impact_statement": "If multiple nodes require repair, Kubernetes Engine might repair them in parallel. Kubernetes Engine limits number of repairs depending on the size of the cluster (bigger clusters have a higher limit) and the number of broken nodes in the cluster (limit decreases if many nodes are broken).\nNode auto-repair is not available on Alpha Clusters.",
    "audit_procedure": "Using Google Cloud Console\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list\nFrom the list of clusters, select the desired cluster. For each Node pool, view the Node pool Details pane and ensure that under the 'Management' heading, 'Auto-repair' is set to 'Enabled'.\n\nUsing Command Line:\nTo check the existence of node auto-repair for an existing cluster's node pool, run:\ngcloud container node-pools describe <node_pool_name> --cluster <cluster_name> --zone <compute_zone> --format json | jq '.management'\n\nEnsure the output of the above command has JSON key attribute autoRepair set to true:\n{\n  \"autoRepair\": true\n}",
    "remediation_procedure": "Using Google Cloud Console\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list\nSelect the Kubernetes cluster containing the node pool for which auto-repair is disabled.\nSelect the Node pool by clicking on the name of the pool.\nNavigate to the Node pool details pane and click EDIT.\nUnder the Management heading, check the Enable auto-repair box.\nClick SAVE.\nRepeat steps 2-6 for every cluster and node pool with auto-upgrade disabled.\n\nUsing Command Line\nTo enable node auto-repair for an existing cluster's Node pool:\ngcloud container node-pools update <node_pool_name> --cluster <cluster_name> --zone <compute_zone> --enable-autorepair",
    "default_value": "Node auto-repair is enabled by default."
  },
  {
    "control_id": "7.6",
    "control": "Perform Automated Vulnerability Scans of Externally-Exposed Enterprise Assets",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889462,
    "recommendation_id": 4683024,
    "view_level": "5.5.6",
    "title": "Ensure Integrity Monitoring for Shielded GKE Nodes is Enabled",
    "pivot_control_id": 424,
    "pivot_recommendation_id": 4683024,
    "url": "https://workbench.cisecurity.org/sections/2889462/recommendations/4683024",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Enable Integrity Monitoring for Shielded GKE Nodes to be notified of inconsistencies during the node boot sequence.",
    "rationale_statement": "Integrity Monitoring provides active alerting for Shielded GKE nodes which allows administrators to respond to integrity failures and prevent compromised nodes from being deployed into the cluster.",
    "impact_statement": "None.",
    "audit_procedure": "Using Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list\nFrom the list of clusters, click on the name of the cluster under test.\nOpen the Details pane for each Node pool within the cluster, and ensure that 'Integrity monitoring' is set to 'Enabled' under the Security heading.\n\nUsing Command Line:\nTo check if Integrity Monitoring is enabled for the Node pools in the cluster, run the following command for each Node pool:\ngcloud container node-pools describe <node_pool_name> --cluster <cluster_name> --zone <compute_zone> --format json | jq .config.shieldedInstanceConfig\n\nThis will return the following, if Integrity Monitoring is enabled:\n{\n   \"enableIntegrityMonitoring\": true\n}",
    "remediation_procedure": "Once a Node pool is provisioned, it cannot be updated to enable Integrity Monitoring. New Node pools must be created within the cluster with Integrity Monitoring enabled.\nUsing Google Cloud Console\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list\nFrom the list of clusters, click on the cluster requiring the update and click ADD NODE POOL.\nEnsure that the 'Integrity monitoring' checkbox is checked under the 'Shielded options' Heading.\nClick SAVE.\n\nWorkloads from existing non-conforming Node pools will need to be migrated to the newly created Node pool, then delete non-conforming Node pools to complete the remediation\nUsing Command Line\nTo create a Node pool within the cluster with Integrity Monitoring enabled, run the following command:\ngcloud container node-pools create <node_pool_name> --cluster <cluster_name> --zone <compute_zone> --shielded-integrity-monitoring\n\nWorkloads from existing non-conforming Node pools will need to be migrated to the newly created Node pool, then delete non-conforming Node pools to complete the remediation",
    "default_value": "Integrity Monitoring is disabled by default on GKE clusters. Integrity Monitoring is enabled by default for Shielded GKE Nodes; however, if Secure Boot is enabled at creation time, Integrity Monitoring is disabled."
  },
  {
    "control_id": "7.6",
    "control": "Perform Automated Vulnerability Scans of Externally-Exposed Enterprise Assets",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889462,
    "recommendation_id": 4683026,
    "view_level": "5.5.7",
    "title": "Ensure Secure Boot for Shielded GKE Nodes is Enabled",
    "pivot_control_id": 424,
    "pivot_recommendation_id": 4683026,
    "url": "https://workbench.cisecurity.org/sections/2889462/recommendations/4683026",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Enable Secure Boot for Shielded GKE Nodes to verify the digital signature of node boot components.",
    "rationale_statement": "An attacker may seek to alter boot components to persist malware or root kits during system initialisation. Secure Boot helps ensure that the system only runs authentic software by verifying the digital signature of all boot components, and halting the boot process if signature verification fails.",
    "impact_statement": "Secure Boot will not permit the use of third-party unsigned kernel modules.",
    "audit_procedure": "Using Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\nFrom the list of clusters, click on the name of the cluster under test.\nOpen the Details pane for each Node pool within the cluster, and ensure that Secure boot is set to Enabled under the Security heading.\n\nUsing Command Line:\nTo check if Secure Boot is enabled for the Node pools in the cluster, run the following command for each Node pool:\ngcloud container node-pools describe <node_pool_name> --cluster <cluster_name> --zone <compute_zone> --format json | jq .config.shieldedInstanceConfig\n\nThis will return the value below, if Secure Boot is enabled:\n{\n  \"enableSecureBoot\": true\n}",
    "remediation_procedure": "Once a Node pool is provisioned, it cannot be updated to enable Secure Boot. New Node pools must be created within the cluster with Secure Boot enabled.\nUsing Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list\nFrom the list of clusters, click on the cluster requiring the update and click ADD NODE POOL.\nEnsure that the Secure boot checkbox is checked under the Shielded options Heading.\nClick SAVE.\n\nWorkloads will need to be migrated from existing non-conforming Node pools to the newly created Node pool, then delete the non-conforming pools.\nUsing Command Line:\nTo create a Node pool within the cluster with Secure Boot enabled, run the following command:\ngcloud container node-pools create <node_pool_name> --cluster <cluster_name> --zone <compute_zone> --shielded-secure-boot\n\nWorkloads will need to be migrated from existing non-conforming Node pools to the newly created Node pool, then delete the non-conforming pools.",
    "default_value": "By default, Secure Boot is disabled in GKE clusters. By default, Secure Boot is disabled when Shielded GKE Nodes is enabled."
  },
  {
    "control_id": "8.2",
    "control": "Collect Audit Logs",
    "IG1": "o",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889456,
    "recommendation_id": 4683003,
    "view_level": "3.2.7",
    "title": "Ensure that the --eventRecordQPS argument is set to 0 or a level which ensures appropriate event capture",
    "pivot_control_id": 428,
    "pivot_recommendation_id": 4683003,
    "url": "https://workbench.cisecurity.org/sections/2889456/recommendations/4683003",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Security relevant information should be captured.  The eventRecordQPS on the Kubelet configuration can be used to limit the rate at which events are gathered and sets the maximum event creations per second.  Setting this too low could result in relevant events not being logged, however the unlimited setting of 0 could result in a denial of service on the kubelet.",
    "rationale_statement": "It is important to capture all events and not restrict event creation. Events are an important source of security information and analytics that ensure that your environment is consistently monitored using the event data.",
    "impact_statement": "Setting this parameter to 0 could result in a denial of service condition due to excessive events being created.  The cluster's event processing and storage systems should be scaled to handle expected event loads.",
    "audit_procedure": "Run the following command on each node:\nsudo grep \"eventRecordQPS\" /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\n\nReview the value set for the argument and determine whether this has been set to an appropriate level for the cluster.\nIf the argument does not exist, check that there is a Kubelet config file specified by --config and review the value in this location.",
    "remediation_procedure": "If using a Kubelet config file, edit the file to set eventRecordQPS: to an appropriate level.\nIf using command line arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable.\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service",
    "default_value": "See the GKE documentation for the default value."
  },
  {
    "control_id": "8.2",
    "control": "Collect Audit Logs",
    "IG1": "o",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889466,
    "recommendation_id": 4683038,
    "view_level": "5.7.1",
    "title": "Ensure Logging and Cloud Monitoring is Enabled",
    "pivot_control_id": 428,
    "pivot_recommendation_id": 4683038,
    "url": "https://workbench.cisecurity.org/sections/2889466/recommendations/4683038",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Send logs and metrics to a remote aggregator to mitigate the risk of local tampering in the event of a breach.",
    "rationale_statement": "Exporting logs and metrics to a dedicated, persistent datastore such as Cloud Operations for GKE ensures availability of audit data following a cluster security event, and provides a central location for analysis of log and metric data collated from multiple sources.",
    "impact_statement": "",
    "audit_procedure": "Using Google Cloud Console:\nLOGGING AND CLOUD MONITORING SUPPORT (PREFERRED):\n\nGo to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\nFrom the list of clusters, click on the cluster of interest.\nUnder the details pane, within the Features section, ensure that Logging is Enabled.\nAlso ensure that Cloud Monitoring  is Enabled\n\nLEGACY STACKDRIVER SUPPORT:\nThis option cannot be check in the GCP console.\nUsing Command Line:\nLOGGING AND CLOUD MONITORING SUPPORT (PREFERRED):\nRun the following commands:\ngcloud container clusters describe <cluster_name> --zone <compute_zone> --format json | jq '.loggingService'\n\ngcloud container clusters describe <cluster_name> --zone <compute_zone> --format json | jq '.monitoringService'\n\nThe output of the above commands should return logging.googleapis.com/kubernetes and monitoring.googleapis.com/kubernetes respectively if Logging and Cloud Monitoring is Enabled.\nLEGACY STACKDRIVER SUPPORT:\nNote: This functionality was decommissioned on 31st March 2021, kept here for posterity (see: https://cloud.google.com/stackdriver/docs/deprecations/legacy for more information)\nBoth Logging and Monitoring support must be enabled.\nFor Logging, run the following command:\ngcloud container clusters describe <cluster_name> --zone <compute_zone> --format json | jq '.loggingService'\n\nThe output should return logging.googleapis.com if Legacy Stackdriver Logging is Enabled.\nFor Monitoring, run the following command:\ngcloud container clusters describe <cluster_name> --zone <compute_zone> --format json | jq '.monitoringService'\n\nThe output should return monitoring.googleapis.com if Legacy Stackdriver Monitoring is Enabled.",
    "remediation_procedure": "Using Google Cloud Console:\nTo enable Logging:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\nSelect the cluster for which Logging is disabled.\nUnder the details pane, within the Features section, click on the pencil icon named Edit logging.\nCheck the box next to Enable Logging.\nIn the drop-down Components box, select the components to be logged.\nClick SAVE CHANGES, and wait for the cluster to update.\n\nTo enable Cloud Monitoring:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\nSelect the cluster for which Logging is disabled.\nUnder the details pane, within the Features section, click on the pencil icon named Edit Cloud Monitoring.\nCheck the box next to Enable Cloud Monitoring.\nIn the drop-down Components box, select the components to be logged.\nClick SAVE CHANGES, and wait for the cluster to update.\n\nUsing Command Line:\nTo enable Logging for an existing cluster, run the following command:\ngcloud container clusters update <cluster_name> --zone <compute_zone> --logging=<components_to_be_logged>\nSee https://cloud.google.com/sdk/gcloud/reference/container/clusters/update#--logging for a list of available components for logging.\nTo enable Cloud Monitoring for an existing cluster, run the following command:\ngcloud container clusters update <cluster_name> --zone <compute_zone> --monitoring=<components_to_be_logged>\nSee https://cloud.google.com/sdk/gcloud/reference/container/clusters/update#--monitoring for a list of available components for Cloud Monitoring.",
    "default_value": "Logging and Cloud Monitoring is enabled by default starting in GKE version 1.14; Legacy Logging and Monitoring support is enabled by default for earlier versions."
  },
  {
    "control_id": "8.2",
    "control": "Collect Audit Logs",
    "IG1": "o",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889466,
    "recommendation_id": 4683041,
    "view_level": "5.7.2",
    "title": "Enable Linux auditd logging",
    "pivot_control_id": 428,
    "pivot_recommendation_id": 4683041,
    "url": "https://workbench.cisecurity.org/sections/2889466/recommendations/4683041",
    "assessment_status": "Manual",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Run the auditd logging daemon to obtain verbose operating system logs from GKE nodes running Container-Optimized OS (COS).",
    "rationale_statement": "Auditd logs provide valuable information about the state of the cluster and workloads, such as error messages, login attempts, and binary executions. This information can be used to debug issues or to investigate security incidents.",
    "impact_statement": "Increased logging activity on a node increases resource usage on that node, which may affect the performance of the workload and may incur additional resource costs. Audit logs sent to Stackdriver consume log quota from the project. The log quota may require increasing and storage to accommodate the additional logs.\nNote that the provided logging daemonset only works on nodes running Container-Optimized OS (COS).",
    "audit_procedure": "Using Google Cloud Console\n\nNavigate to the Kubernetes Engine workloads by visiting: https://console.cloud.google.com/kubernetes/workload\nObserve the workloads and ensure that all filters are removed.\nIf the unmodified example auditd logging daemonset: https://raw.githubusercontent.com/GoogleCloudPlatform/k8s-node-tools/master/os-audit/cos-auditd-logging.yaml is being used, ensure that the cos-auditd-logging daemonset is being run in the cos-auditd namespace with the number of running pods reporting as expected.\n\nUsing Command Line:\nIf using the unmodified example auditd logging daemonset, run:\nkubectl get daemonsets -n cos-audit\n\nand observe that the cos-auditd-logging daemonset is running as expected.\nIf the name or namespace of the daemonset has been modified and is unknown, search for the container being used by the daemonset:\nkubectl get daemonsets -A -o json | jq '.items[] | select (.spec.template.spec.containers[].image | contains (\"gcr.io/stackdriver-agents/stackdriver-logging-agent\"))'| jq '{name: .metadata.name, annotations: .metadata.annotations.\"kubernetes.io/description\", namespace: .metadata.namespace, status: .status}'\n\nThe above command returns the name, namespace and status of the daemonsets that use the Stackdriver logging agent. The example auditd logging daemonset has a description within the annotation as output by the command above:\n{\n  \"name\": \"cos-auditd-logging\",\n  \"annotations\": \"DaemonSet that enables Linux auditd logging on COS nodes.\",\n  \"namespace\": \"cos-auditd\",\n  \"status\": {...\n  }\n}\n\nEnsure that the status fields return that the daemonset is running as expected.",
    "remediation_procedure": "Using Command Line:\nDownload the example manifests:\ncurl https://raw.githubusercontent.com/GoogleCloudPlatform/k8s-node-tools/master/os-audit/cos-auditd-logging.yaml > cos-auditd-logging.yaml\n\nEdit the example manifests if needed. Then, deploy them:\nkubectl apply -f cos-auditd-logging.yaml\n\nVerify that the logging Pods have started. If a different Namespace was defined in the manifests, replace cos-auditd with the name of the namespace being used:\nkubectl get pods --namespace=cos-auditd",
    "default_value": "By default, the auditd logging daemonset is not launched when a GKE cluster is created."
  },
  {
    "control_id": "8.5",
    "control": "Collect Detailed Audit Logs",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889456,
    "recommendation_id": 4683003,
    "view_level": "3.2.7",
    "title": "Ensure that the --eventRecordQPS argument is set to 0 or a level which ensures appropriate event capture",
    "pivot_control_id": 431,
    "pivot_recommendation_id": 4683003,
    "url": "https://workbench.cisecurity.org/sections/2889456/recommendations/4683003",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Security relevant information should be captured.  The eventRecordQPS on the Kubelet configuration can be used to limit the rate at which events are gathered and sets the maximum event creations per second.  Setting this too low could result in relevant events not being logged, however the unlimited setting of 0 could result in a denial of service on the kubelet.",
    "rationale_statement": "It is important to capture all events and not restrict event creation. Events are an important source of security information and analytics that ensure that your environment is consistently monitored using the event data.",
    "impact_statement": "Setting this parameter to 0 could result in a denial of service condition due to excessive events being created.  The cluster's event processing and storage systems should be scaled to handle expected event loads.",
    "audit_procedure": "Run the following command on each node:\nsudo grep \"eventRecordQPS\" /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\n\nReview the value set for the argument and determine whether this has been set to an appropriate level for the cluster.\nIf the argument does not exist, check that there is a Kubelet config file specified by --config and review the value in this location.",
    "remediation_procedure": "If using a Kubelet config file, edit the file to set eventRecordQPS: to an appropriate level.\nIf using command line arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf on each worker node and set the below parameter in KUBELET_SYSTEM_PODS_ARGS variable.\nBased on your system, restart the kubelet service. For example:\nsystemctl daemon-reload\nsystemctl restart kubelet.service",
    "default_value": "See the GKE documentation for the default value."
  },
  {
    "control_id": "8.5",
    "control": "Collect Detailed Audit Logs",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889463,
    "recommendation_id": 4683027,
    "view_level": "5.6.1",
    "title": "Enable VPC Flow Logs and Intranode Visibility",
    "pivot_control_id": 431,
    "pivot_recommendation_id": 4683027,
    "url": "https://workbench.cisecurity.org/sections/2889463/recommendations/4683027",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Enable VPC Flow Logs and Intranode Visibility to see pod-level traffic, even for traffic within a worker node.",
    "rationale_statement": "Enabling Intranode Visibility makes intranode pod to pod traffic visible to the networking fabric. With this feature, VPC Flow Logs or other VPC features can be used for intranode traffic.",
    "impact_statement": "Enabling it on existing cluster causes the cluster master and the cluster nodes to restart, which might cause disruption.",
    "audit_procedure": "Using Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list\nSelect the desired cluster, and under the Cluster section, make sure that Intranode visibility is set to Enabled.\n\nUsing Command Line:\nRun this command:\ngcloud container clusters describe <cluster_name> --zone <compute_zone> --format json | jq '.networkConfig.enableIntraNodeVisibility'\n\nThe result should return true if Intranode Visibility is Enabled.",
    "remediation_procedure": "Enable Intranode Visibility:\nUsing Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\nSelect Kubernetes clusters for which intranode visibility is disabled.\nWithin the Details pane, under the Network section, click on the pencil icon named Edit intranode visibility.\nCheck the box next to Enable Intranode visibility.\nClick SAVE CHANGES.\n\nUsing Command Line:\nTo enable intranode visibility on an existing cluster, run the following command:\ngcloud container clusters update <cluster_name> --enable-intra-node-visibility\n\nEnable VPC Flow Logs:\nUsing Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\nSelect Kubernetes clusters for which VPC Flow Logs are disabled.\nSelect Nodes tab.\nSelect Node Pool without VPC Flow Logs enabled.\nSelect an Instance Group within the node pool.\nSelect an Instance Group Member.\nSelect the Subnetwork under Network Interfaces.\nClick on EDIT.\nSet Flow logs to On.\nClick SAVE.\n\nUsing Command Line:\n\nFind the subnetwork name associated with the cluster.\n\ngcloud container clusters describe <cluster_name> --region <cluster_region> --format json | jq '.subnetwork'\n\n\nUpdate the subnetwork to enable VPC Flow Logs.\n\ngcloud compute networks subnets update <subnet_name> --enable-flow-logs",
    "default_value": "By default, Intranode Visibility is disabled."
  },
  {
    "control_id": "12.2",
    "control": "Establish and Maintain a Secure Network Architecture",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889470,
    "recommendation_id": 4683051,
    "view_level": "4.6.4",
    "title": "The default namespace should not be used",
    "pivot_control_id": 462,
    "pivot_recommendation_id": 4683051,
    "url": "https://workbench.cisecurity.org/sections/2889470/recommendations/4683051",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Kubernetes provides a default namespace, where objects are placed if no namespace is specified for them.  Placing objects in this namespace makes application of RBAC and other controls more difficult.",
    "rationale_statement": "Resources in a Kubernetes cluster should be segregated by namespace, to allow for security controls to be applied at that level and to make it easier to manage resources.",
    "impact_statement": "None",
    "audit_procedure": "Run this command to list objects in default namespace\nkubectl get $(kubectl api-resources --verbs=list --namespaced=true -o name | paste -sd, -) --ignore-not-found -n default\n\nThe only entries there should be system managed resources such as the kubernetes service\nOR\nkubectl get pods -n default\n\nReturning No resources found in default namespace.",
    "remediation_procedure": "Ensure that namespaces are created to allow for appropriate segregation of Kubernetes resources and that all new resources are created in a specific namespace.",
    "default_value": "Unless a namespace is specific on object creation, the default namespace will be used"
  },
  {
    "control_id": "12.3",
    "control": "Securely Manage Network Infrastructure",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889456,
    "recommendation_id": 4683002,
    "view_level": "3.2.6",
    "title": "Ensure that the --make-iptables-util-chains argument is set to true",
    "pivot_control_id": 463,
    "pivot_recommendation_id": 4683002,
    "url": "https://workbench.cisecurity.org/sections/2889456/recommendations/4683002",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Allow Kubelet to manage iptables.",
    "rationale_statement": "Kubelets can automatically manage the required changes to iptables based on how you choose your networking options for the pods. It is recommended to let kubelets manage the changes to iptables. This ensures that the iptables configuration remains in sync with pods networking configuration. Manually configuring iptables with dynamic pod network configuration changes might hamper the communication between pods/containers and to the outside world. You might have iptables rules too restrictive or too open.",
    "impact_statement": "Kubelet would manage the iptables on the system and keep it in sync. If you are using any other iptables management solution, then there might be some conflicts.",
    "audit_procedure": "Audit Method 1:\nFirst, SSH to each node:\nRun the following command on each node to find the Kubelet process:\nps -ef | grep kubelet\n\nIf the output of the above command includes the argument --make-iptables-util-chains then verify it is set to true.\nIf the --make-iptables-util-chains argument does not exist, and there is a Kubelet config file specified by --config, verify that the file does not set makeIPTablesUtilChains to false.\nAudit Method 2:\nIf using the api configz endpoint consider searching for the status of authentication... \"makeIPTablesUtilChains.:true by extracting the live configuration from the nodes running kubelet.\nSet the local proxy port and the following variables and provide proxy port number and node name;\nHOSTNAME_PORT=\"localhost-and-port-number\"\nNODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\"\nkubectl proxy --port=8001 &\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=gke-cluster-1-pool1-5e572947-r2hg (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"",
    "remediation_procedure": "Remediation Method 1:\nIf modifying the Kubelet config file, edit the kubelet-config.json file /etc/kubernetes/kubelet/kubelet-config.json and set the below parameter to true\n\"makeIPTablesUtilChains\": true\n\nEnsure that /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf does not set the --make-iptables-util-chains argument because that would override your Kubelet config file.\nRemediation Method 2:\nIf using executable arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf on each worker node and add the below parameter at the end of the KUBELET_ARGS variable string.\n--make-iptables-util-chains:true\n\nRemediation Method 3:\nIf using the api configz endpoint consider searching for the status of \"makeIPTablesUtilChains.: true by extracting the live configuration from the nodes running kubelet.\n**See detailed step-by-step configmap procedures in Reconfigure a Node's Kubelet in a Live Cluster, and then rerun the curl statement from audit process to check for kubelet configuration changes\nkubectl proxy --port=8001 &\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=gke-cluster-1-pool1-5e572947-r2hg (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\nFor all three remediations:\nBased on your system, restart the kubelet service and check status\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l",
    "default_value": "See the GKE documentation for the default value."
  },
  {
    "control_id": "12.6",
    "control": "Use of Secure Network Management and Communication Protocols",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889456,
    "recommendation_id": 4682998,
    "view_level": "3.2.4",
    "title": "Ensure that the --read-only-port is disabled",
    "pivot_control_id": 466,
    "pivot_recommendation_id": 4682998,
    "url": "https://workbench.cisecurity.org/sections/2889456/recommendations/4682998",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Disable the read-only port.",
    "rationale_statement": "The Kubelet process provides a read-only API in addition to the main Kubelet API. Unauthenticated access is provided to this read-only API which could possibly retrieve potentially sensitive information about the cluster.",
    "impact_statement": "Removal of the read-only port will require that any service which made use of it will need to be re-configured to use the main Kubelet API.",
    "audit_procedure": "If using a Kubelet configuration file, check that there is an entry for authentication: anonymous: enabled set to 0.\nFirst, SSH to the relevant node:\nRun the following command on each node to find the appropriate Kubelet config file:\nps -ef | grep kubelet\n\nThe output of the above command should return something similar to --config /etc/kubernetes/kubelet/kubelet-config.json which is the location of the Kubelet config file.\nOpen the Kubelet config file:\ncat /etc/kubernetes/kubelet/kubelet-config.json\n\nVerify that the --read-only-port argument exists and is set to 0.\nIf the --read-only-port argument is not present, check that there is a Kubelet config file specified by --config. Check that if there is a readOnlyPort entry in the file, it is set to 0.",
    "remediation_procedure": "If modifying the Kubelet config file, edit the kubelet-config.json file /etc/kubernetes/kubelet/kubelet-config.json and set the below parameter to 0\n\"readOnlyPort\": 0\n\nIf using executable arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf on each worker node and add the below parameter at the end of the KUBELET_ARGS variable string.\n--read-only-port=0\n\nFor each remediation:\nBased on your system, restart the kubelet service and check status\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l",
    "default_value": "See the GKE documentation for the default value."
  },
  {
    "control_id": "12.6",
    "control": "Use of Secure Network Management and Communication Protocols",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889456,
    "recommendation_id": 4683000,
    "view_level": "3.2.5",
    "title": "Ensure that the --streaming-connection-idle-timeout argument is not set to 0",
    "pivot_control_id": 466,
    "pivot_recommendation_id": 4683000,
    "url": "https://workbench.cisecurity.org/sections/2889456/recommendations/4683000",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Do not disable timeouts on streaming connections.",
    "rationale_statement": "Setting idle timeouts ensures that you are protected against Denial-of-Service attacks, inactive connections and running out of ephemeral ports.\nNote: By default, --streaming-connection-idle-timeout is set to 4 hours which might be too high for your environment. Setting this as appropriate would additionally ensure that such streaming connections are timed out after serving legitimate use cases.",
    "impact_statement": "Long-lived connections could be interrupted.",
    "audit_procedure": "Audit Method 1:\nFirst, SSH to the relevant node:\nRun the following command on each node to find the running kubelet process:\nps -ef | grep kubelet\n\nIf the command line for the process includes the argument streaming-connection-idle-timeout verify that it is not set to 0.\nIf the streaming-connection-idle-timeout argument is not present in the output of the above command, refer instead to the config argument that specifies the location of the Kubelet config file e.g. --config /etc/kubernetes/kubelet-config.yaml.\nOpen the Kubelet config file:\ncat /etc/kubernetes/kubelet-config.yaml\n\nVerify that the streamingConnectionIdleTimeout argument is not set to 0.\nAudit Method 2:\nIf using the api configz endpoint consider searching for the status of \"streamingConnectionIdleTimeout\":\"4h0m0s\" by extracting the live configuration from the nodes running kubelet.\nSet the local proxy port and the following variables and provide proxy port number and node name;\nHOSTNAME_PORT=\"localhost-and-port-number\"\nNODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\"\nkubectl proxy --port=8001 &\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=gke-cluster-1-pool1-5e572947-r2hg (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"",
    "remediation_procedure": "Remediation Method 1:\nIf modifying the Kubelet config file, edit the kubelet-config.json file /etc/kubernetes/kubelet-config.yaml and set the below parameter to a non-zero value in the format of #h#m#s\n\"streamingConnectionIdleTimeout\": \"4h0m0s\"\n\nYou should ensure that the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf does not specify a --streaming-connection-idle-timeout argument because it would override the Kubelet config file.\nRemediation Method 2:\nIf using executable arguments, edit the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf on each worker node and add the below parameter at the end of the KUBELET_ARGS variable string.\n--streaming-connection-idle-timeout=4h0m0s\n\nRemediation Method 3:\nIf using the api configz endpoint consider searching for the status of \"streamingConnectionIdleTimeout\": by extracting the live configuration from the nodes running kubelet.\n**See detailed step-by-step configmap procedures in Reconfigure a Node's Kubelet in a Live Cluster, and then rerun the curl statement from audit process to check for kubelet configuration changes\nkubectl proxy --port=8001 &\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=gke-cluster-1-pool1-5e572947-r2hg (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\nFor all three remediations:\nBased on your system, restart the kubelet service and check status\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l",
    "default_value": "See the GKE documentation for the default value."
  },
  {
    "control_id": "13",
    "control": "Network Monitoring and Defense",
    "IG1": "-",
    "IG2": "-",
    "IG3": "-",
    "section_id": 2889470,
    "recommendation_id": 4683046,
    "view_level": "4.6.1",
    "title": "Create administrative boundaries between resources using namespaces",
    "pivot_control_id": 469,
    "pivot_recommendation_id": 4683046,
    "url": "https://workbench.cisecurity.org/sections/2889470/recommendations/4683046",
    "assessment_status": "Manual",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Use namespaces to isolate your Kubernetes objects.",
    "rationale_statement": "Limiting the scope of user permissions can reduce the impact of mistakes or malicious activities. A Kubernetes namespace allows you to partition created resources into logically named groups. Resources created in one namespace can be hidden from other namespaces. By default, each resource created by a user in Kubernetes cluster runs in a default namespace, called default. You can create additional namespaces and attach resources and users to them. You can use Kubernetes Authorization plugins to create policies that segregate access to namespace resources between different users.",
    "impact_statement": "You need to switch between namespaces for administration.",
    "audit_procedure": "Run the below command and review the namespaces created in the cluster.\nkubectl get namespaces\n\nEnsure that these namespaces are the ones you need and are adequately administered as per your requirements.",
    "remediation_procedure": "Follow the documentation and create namespaces for objects in your deployment as you need them.",
    "default_value": "By default, Kubernetes starts with two initial namespaces:\n\ndefault - The default namespace for objects with no other namespace\nkube-system - The namespace for objects created by the Kubernetes system\nkube-node-lease - Namespace used for node heartbeats\nkube-public - Namespace used for public information in a cluster"
  },
  {
    "control_id": "13.4",
    "control": "Perform Traffic Filtering Between Network Segments",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889465,
    "recommendation_id": 4683035,
    "view_level": "4.3.2",
    "title": "Ensure that all Namespaces have Network Policies defined",
    "pivot_control_id": 473,
    "pivot_recommendation_id": 4683035,
    "url": "https://workbench.cisecurity.org/sections/2889465/recommendations/4683035",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Use network policies to isolate traffic in the cluster network.",
    "rationale_statement": "Running different applications on the same Kubernetes cluster creates a risk of one compromised application attacking a neighboring application. Network segmentation is important to ensure that containers can communicate only with those they are supposed to. A network policy is a specification of how selections of pods are allowed to communicate with each other and other network endpoints.\nNetwork Policies are namespace scoped.  When a network policy is introduced to a given namespace, all traffic not allowed by the policy is denied. However, if there are no network policies in a namespace all traffic will be allowed into and out of the pods in that namespace.",
    "impact_statement": "Once network policies are in use within a given namespace, traffic not explicitly allowed by a network policy will be denied.  As such it is important to ensure that, when introducing network policies, legitimate traffic is not blocked.",
    "audit_procedure": "Run the below command and review the NetworkPolicy objects created in the cluster.\nkubectl get networkpolicy --all-namespaces\n\nensure that each namespace defined in the cluster has at least one Network Policy.",
    "remediation_procedure": "Follow the documentation and create NetworkPolicy objects as needed.\nSee: https://cloud.google.com/kubernetes-engine/docs/how-to/network-policy#creating_a_network_policy for more information.",
    "default_value": "By default, network policies are not created."
  },
  {
    "control_id": "13.4",
    "control": "Perform Traffic Filtering Between Network Segments",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889463,
    "recommendation_id": 4683029,
    "view_level": "5.6.2",
    "title": "Ensure use of VPC-native clusters",
    "pivot_control_id": 473,
    "pivot_recommendation_id": 4683029,
    "url": "https://workbench.cisecurity.org/sections/2889463/recommendations/4683029",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Create Alias IPs for the node network CIDR range in order to subsequently configure IP-based policies and firewalling for pods. A cluster that uses Alias IPs is called a VPC-native cluster.",
    "rationale_statement": "Using Alias IPs has several benefits:\n\nPod IPs are reserved within the network ahead of time, which prevents conflict with other compute resources.\nThe networking layer can perform anti-spoofing checks to ensure that egress traffic is not sent with arbitrary source IPs.\nFirewall controls for Pods can be applied separately from their nodes.\nAlias IPs allow Pods to directly access hosted services without using a NAT gateway.",
    "impact_statement": "You cannot currently migrate an existing cluster that uses routes for Pod routing to a cluster that uses Alias IPs.\nCluster IPs for internal services remain only available from within the cluster. If you want to access a Kubernetes Service from within the VPC, but from outside of the cluster, use an internal load balancer.",
    "audit_procedure": "Using Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\nFrom the list of clusters, click on the desired cluster to open the Details page. Under the 'Networking' section, make sure 'VPC-native traffic routing' is set to 'Enabled'.\n\nUsing Command Line:\nTo check Alias IP is enabled for an existing cluster, run the following command:\ngcloud container clusters describe <cluster_name> --zone <compute_zone> --format json | jq '.ipAllocationPolicy.useIpAliases'\n\nThe output of the above command should return true, if VPC-native (using alias IP) is enabled. If VPC-native (using alias IP) is disabled, the above command will return null ({ }).",
    "remediation_procedure": "Alias IPs cannot be enabled on an existing cluster. To create a new cluster using Alias IPs, follow the instructions below.\nUsing Google Cloud Console:\nIf using Standard configuration mode:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list\nClick CREATE CLUSTER, and select Standard configuration mode.\nConfigure your cluster as desired , then, click Networking under CLUSTER in the navigation pane.\nIn the 'VPC-native' section, leave 'Enable VPC-native (using alias IP)' selected\nClick CREATE.\n\nIf using Autopilot configuration mode:\nNote that this is VPC-native only and cannot be disable:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\nClick CREATE CLUSTER, and select Autopilot configuration mode.\nConfigure your cluster as required\nClick CREATE.\n\nUsing Command Line\nTo enable Alias IP on a new cluster, run the following command:\ngcloud container clusters create <cluster_name> --zone <compute_zone> --enable-ip-alias\n\nIf using Autopilot configuration mode:\ngcloud container clusters create-auto <cluster_name> --zone <compute_zone>",
    "default_value": "By default, VPC-native (using alias IP) is enabled when you create a new cluster in the Google Cloud Console, however this is disabled when creating a new cluster using the gcloud CLI, unless the --enable-ip-alias argument is specified."
  },
  {
    "control_id": "16.5",
    "control": "Use Up-to-Date and Trusted Third-Party Software Components",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889465,
    "recommendation_id": 4683034,
    "view_level": "4.3.1",
    "title": "Ensure that the CNI in use supports Network Policies",
    "pivot_control_id": 503,
    "pivot_recommendation_id": 4683034,
    "url": "https://workbench.cisecurity.org/sections/2889465/recommendations/4683034",
    "assessment_status": "Manual",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "There are a variety of CNI plugins available for Kubernetes. If the CNI in use does not support Network Policies it may not be possible to effectively restrict traffic in the cluster.",
    "rationale_statement": "Kubernetes network policies are enforced by the CNI plugin in use.  As such it is important to ensure that the CNI plugin supports both Ingress and Egress network policies.\nSee also recommendation 5.6.7.",
    "impact_statement": "None",
    "audit_procedure": "Review the documentation of CNI plugin in use by the cluster, and confirm that it supports Ingress and Egress network policies.",
    "remediation_procedure": "To use a CNI plugin with Network Policy, enable Network Policy in GKE, and the CNI plugin will be updated. See recommendation 5.6.7.",
    "default_value": "This will depend on the CNI plugin in use."
  },
  {
    "control_id": "16.7",
    "control": "Use Standard Hardening Configuration Templates for Application Infrastructure",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889464,
    "recommendation_id": 4683032,
    "view_level": "4.2.1",
    "title": "Ensure that the cluster enforces Pod Security Standard Baseline profile or stricter for all namespaces.",
    "pivot_control_id": 505,
    "pivot_recommendation_id": 4683032,
    "url": "https://workbench.cisecurity.org/sections/2889464/recommendations/4683032",
    "assessment_status": "Manual",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "The Pod Security Standard Baseline profile defines a baseline for container security. You can enforce this by using the built-in Pod Security Admission controller.",
    "rationale_statement": "Without an active mechanism to enforce the Pod Security Standard Baseline profile, it is not possible to limit the use of containers with access to underlying cluster nodes, via mechanisms like privileged containers, or the use of hostPath volume mounts.",
    "impact_statement": "Enforcing a baseline profile will limit the use of containers.",
    "audit_procedure": "diff \n<(kubectl get namespace -l pod-security.kubernetes.io/enforce=baseline -o jsonpath='{range .items[]}{.metadata.name}{\"\\n\"}') \n<(kubectl get namespace -o jsonpath='{range .items[]}{.metadata.name}{\"\\n\"}')",
    "remediation_procedure": "Ensure that Pod Security Admission is in place for every namespace which contains user workloads.\nRun the following command to enforce the Baseline profile in a namespace:\nkubectl label namespace  pod-security.kubernetes.io/enforce=baseline",
    "default_value": "By default, Pod Security Admission is enabled but no policies are in place."
  },
  {
    "control_id": "16.7",
    "control": "Use Standard Hardening Configuration Templates for Application Infrastructure",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889470,
    "recommendation_id": 4683047,
    "view_level": "4.6.2",
    "title": "Ensure that the seccomp profile is set to RuntimeDefault in the pod definitions",
    "pivot_control_id": 505,
    "pivot_recommendation_id": 4683047,
    "url": "https://workbench.cisecurity.org/sections/2889470/recommendations/4683047",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Enable RuntimeDefault seccomp profile in the pod definitions.",
    "rationale_statement": "Seccomp (secure computing mode) is used to restrict the set of system calls applications can make, allowing cluster administrators greater control over the security of workloads running in the cluster. Kubernetes disables seccomp profiles by default for historical reasons. It should be enabled to ensure that the workloads have restricted actions available within the container.",
    "impact_statement": "If the RuntimeDefault seccomp profile is too restrictive for you, you would have to create/manage your own Localhost seccomp profiles.",
    "audit_procedure": "Review the pod definitions output for all namespaces in the cluster with the command below.\nkubectl get pods --all-namespaces -o json | jq -r '.items[] | select(.metadata.annotations.\"seccomp.security.alpha.kubernetes.io/pod\" == \"runtime/default\" or .spec.securityContext.seccompProfile.type == \"RuntimeDefault\") | {namespace: .metadata.namespace, name: .metadata.name, seccompProfile: .spec.securityContext.seccompProfile.type}'",
    "remediation_procedure": "Use security context to enable the RuntimeDefault seccomp profile in your pod definitions. An example is as below:\n{\n  \"namespace\": \"kube-system\",\n  \"name\": \"metrics-server-v0.7.0-dbcc8ddf6-gz7d4\",\n  \"seccompProfile\": \"RuntimeDefault\"\n}",
    "default_value": "By default, seccomp profile is set to unconfined which means that no seccomp profiles are enabled."
  },
  {
    "control_id": "16.7",
    "control": "Use Standard Hardening Configuration Templates for Application Infrastructure",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889461,
    "recommendation_id": 4683014,
    "view_level": "5.4.1",
    "title": "Ensure the GKE Metadata Server is Enabled",
    "pivot_control_id": 505,
    "pivot_recommendation_id": 4683014,
    "url": "https://workbench.cisecurity.org/sections/2889461/recommendations/4683014",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Running the GKE Metadata Server prevents workloads from accessing sensitive instance metadata and facilitates Workload Identity.",
    "rationale_statement": "Every node stores its metadata on a metadata server. Some of this metadata, such as kubelet credentials and the VM instance identity token, is sensitive and should not be exposed to a Kubernetes workload. Enabling the GKE Metadata server prevents pods (that are not running on the host network) from accessing this metadata and facilitates Workload Identity.\nWhen unspecified, the default setting allows running pods to have full access to the node's underlying metadata server.",
    "impact_statement": "The GKE Metadata Server must be run when using Workload Identity. Because Workload Identity replaces the need to use Metadata Concealment, the two approaches are incompatible.\nWhen the GKE Metadata Server and Workload Identity are enabled, unless the Pod is running on the host network, Pods cannot use the the Compute Engine default service account.\nWorkloads may need modification in order for them to use Workload Identity as described within: https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity.",
    "audit_procedure": "Using Google Cloud Console\n\nGo to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\nFrom the list of clusters, click on the name of the cluster of interest and for each Node pool within the cluster, open the Details pane, and ensure that the GKE Metadata Server is set to Enabled.\n\nUsing Command Line\nTo check whether the GKE Metadata Server is enabled for each Node pool within a cluster, run the following command:\ngcloud container clusters describe <cluster_name> --zone <cluster_zone> --format json | jq .nodePools[].config.workloadMetadataConfig\n\nThis should return the following for each Node pool:\n{\n    \"mode\": \"GKE_METADATA\"\n}\n\nNull ({ }) is returned if the GKE Metadata Server is not enabled.",
    "remediation_procedure": "The GKE Metadata Server requires Workload Identity to be enabled on a cluster. Modify the cluster to enable Workload Identity and enable the GKE Metadata Server.\nUsing Google Cloud Console\n\nGo to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\nFrom the list of clusters, select the cluster for which Workload Identity is disabled.\nUnder the DETAILS pane, navigate down to the Security subsection.\nClick on the pencil icon named Edit Workload Identity, click on Enable Workload Identity in the pop-up window, and select a workload pool from the drop-down box. By default, it will be the namespace of the Cloud project containing the cluster, for example: <project_id>.svc.id.goog.\nClick SAVE CHANGES and wait for the cluster to update.\nOnce the cluster has updated, select each Node pool within the cluster Details page.\nFor each Node pool, select EDIT within the Node pool details page.\nWithin the Edit node pool pane, check the Enable GKE Metadata Server checkbox.\nClick SAVE.\n\nUsing Command Line\ngcloud container clusters update <cluster_name> --identity-namespace=<project_id>.svc.id.goog\n\nNote that existing Node pools are unaffected. New Node pools default to --workload-metadata-from-node=GKE_METADATA_SERVER.\nTo modify an existing Node pool to enable GKE Metadata Server:\ngcloud container node-pools update <node_pool_name> --cluster=<cluster_name> --workload-metadata-from-node=GKE_METADATA_SERVER\n\nWorkloads may need modification in order for them to use Workload Identity as described within: https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity.",
    "default_value": "By default, running pods to have full access to the node's underlying metadata server."
  },
  {
    "control_id": "16.7",
    "control": "Use Standard Hardening Configuration Templates for Application Infrastructure",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889462,
    "recommendation_id": 4683023,
    "view_level": "5.5.5",
    "title": "Ensure Shielded GKE Nodes are Enabled",
    "pivot_control_id": 505,
    "pivot_recommendation_id": 4683023,
    "url": "https://workbench.cisecurity.org/sections/2889462/recommendations/4683023",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Shielded GKE Nodes provides verifiable integrity via secure boot, virtual trusted platform module (vTPM)-enabled measured boot, and integrity monitoring.",
    "rationale_statement": "Shielded GKE nodes protects clusters against boot- or kernel-level malware or rootkits which persist beyond infected OS.\nShielded GKE nodes run firmware which is signed and verified using Google's Certificate Authority, ensuring that the nodes' firmware is unmodified and establishing the root of trust for Secure Boot. GKE node identity is strongly protected via virtual Trusted Platform Module (vTPM) and verified remotely by the master node before the node joins the cluster. Lastly, GKE node integrity (i.e., boot sequence and kernel) is measured and can be monitored and verified remotely.",
    "impact_statement": "After Shielded GKE Nodes is enabled in a cluster, any nodes created in a Node pool without Shielded GKE Nodes enabled, or created outside of any Node pool, aren't able to join the cluster.\nShielded GKE Nodes can only be used with Container-Optimized OS (COS), COS with containerd, and Ubuntu node images.",
    "audit_procedure": "Using Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list\nSelect the cluster under test from the list of clusters, and ensure that Shielded GKE Nodes are 'Enabled' under the Details pane.\n\nUsing Command Line:\nRun the following command:\ngcloud container clusters describe <cluster_name> --format json | jq '.shieldedNodes'\n\nThis will return the following if Shielded GKE Nodes are enabled:\n{\n  \"enabled\": true\n}",
    "remediation_procedure": "Note: From version 1.18, clusters will have Shielded GKE nodes enabled by default.\nUsing Google Cloud Console:\nTo update an existing cluster to use Shielded GKE nodes:\n\nNavigate to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\nSelect the cluster which for which Shielded GKE Nodes is to be enabled.\nWith in the Details pane, under the Security heading, click on the pencil icon named Edit Shields GKE nodes.\nCheck the box named Enable Shield GKE nodes.\nClick SAVE CHANGES.\n\nUsing Command Line:\nTo migrate an existing cluster, the flag --enable-shielded-nodes needs to be specified in the cluster update command:\ngcloud container clusters update <cluster_name> --zone <cluster_zone> --enable-shielded-nodes",
    "default_value": "Clusters will have Shielded GKE nodes enabled by default, as of version v1.18"
  },
  {
    "control_id": "16.8",
    "control": "Separate Production and Non-Production Systems",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889472,
    "recommendation_id": 4683053,
    "view_level": "5.10.2",
    "title": "Ensure that Alpha clusters are not used for production workloads",
    "pivot_control_id": 506,
    "pivot_recommendation_id": 4683053,
    "url": "https://workbench.cisecurity.org/sections/2889472/recommendations/4683053",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 1"
      }
    ],
    "description": "Alpha clusters are not covered by an SLA and are not production-ready.",
    "rationale_statement": "Alpha clusters are designed for early adopters to experiment with workloads that take advantage of new features before those features are production-ready. They have all Kubernetes API features enabled, but are not covered by the GKE SLA, do not receive security updates, have node auto-upgrade and node auto-repair disabled, and cannot be upgraded. They are also automatically deleted after 30 days.",
    "impact_statement": "Users and workloads will not be able to take advantage of features included within Alpha clusters.",
    "audit_procedure": "The audit script for this recommendation utilizes 3 variables:\n$CLUSTER_NAME\n$COMPUTE_ZONE\nPlease set these parameters on the system where you will be executing your gcloud audit script or command.\nUsing Google Cloud Console\n\nGo to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/list\nIf a cluster appears under the 'Kubernetes alpha clusters' heading, it is an Alpha cluster.\n\nUsing Command Line\nRun the command:\ngcloud container clusters describe $CLUSTER_NAME \\\n  --zone $COMPUTE-ZONE \\\n  --format json | jq '.enableKubernetesAlpha'\n\nThe output of the above command will return true if it is an Alpha cluster.",
    "remediation_procedure": "Alpha features cannot be disabled. To remediate, a new cluster must be created.\nUsing Google Cloud Console\n\nGo to Kubernetes Engine by visiting https://console.cloud.google.com/kubernetes/\nClick CREATE CLUSTER, and choose \"SWITCH TO STANDARD CLUSTER\" in the upper right corner of the screen.\nUnder Features in the the CLUSTER section, \"Enable Kubernetes alpha features in this cluster\" will not be available by default and to use Kubernetes alpha features in this cluster, first disable release channels.\nNote: It will only be available if the cluster is created with a Static version for the Control plane version, along with both Automatically upgrade nodes to the next available version and Enable auto-repair being checked under the Node pool details for each node.\nConfigure the other settings as required and click CREATE.\n\nUsing Command Line:\nUpon creating a new cluster\ngcloud container clusters create [CLUSTER_NAME] \\\n  --zone [COMPUTE_ZONE]\n\nDo not use the --enable-kubernetes-alpha argument.",
    "default_value": "By default, Kubernetes Alpha features are disabled."
  },
  {
    "control_id": "16.8",
    "control": "Separate Production and Non-Production Systems",
    "IG1": "-",
    "IG2": "o",
    "IG3": "o",
    "section_id": 2889472,
    "recommendation_id": 4683054,
    "view_level": "5.10.3",
    "title": "Consider GKE Sandbox for running untrusted workloads",
    "pivot_control_id": 506,
    "pivot_recommendation_id": 4683054,
    "url": "https://workbench.cisecurity.org/sections/2889472/recommendations/4683054",
    "assessment_status": "Automated",
    "applicable_profiles": [
      {
        "title": "Level 2"
      }
    ],
    "description": "Use GKE Sandbox to restrict untrusted workloads as an additional layer of protection when running in a multi-tenant environment.",
    "rationale_statement": "GKE Sandbox provides an extra layer of security to prevent untrusted code from affecting the host kernel on your cluster nodes.\nWhen you enable GKE Sandbox on a Node pool, a sandbox is created for each Pod running on a node in that Node pool. In addition, nodes running sandboxed Pods are prevented from accessing other GCP services or cluster metadata. Each sandbox uses its own userspace kernel.\nMulti-tenant clusters and clusters whose containers run untrusted workloads are more exposed to security vulnerabilities than other clusters. Examples include SaaS providers, web-hosting providers, or other organizations that allow their users to upload and run code. A flaw in the container runtime or in the host kernel could allow a process running within a container to 'escape' the container and affect the node's kernel, potentially bringing down the node.\nThe potential also exists for a malicious tenant to gain access to and exfiltrate another tenant's data in memory or on disk, by exploiting such a defect.",
    "impact_statement": "Using GKE Sandbox requires the node image to be set to Container-Optimized OS with containerd (cos_containerd).\nIt is not currently possible to use GKE Sandbox along with the following Kubernetes features:\n\nAccelerators such as GPUs or TPUs\nIstio\nMonitoring statistics at the level of the Pod or container\nHostpath storage\nPer-container PID namespace\nCPU and memory limits are only applied for Guaranteed Pods and Burstable Pods, and only when CPU and memory limits are specified for all containers running in the Pod\nPods using PodSecurityPolicies that specify host namespaces, such as hostNetwork, hostPID, or hostIPC\nPods using PodSecurityPolicy settings such as privileged mode\nVolumeDevices\nPortforward\nLinux kernel security modules such as Seccomp, Apparmor, or Selinux Sysctl, NoNewPrivileges, bidirectional MountPropagation, FSGroup, or ProcMount",
    "audit_procedure": "Using Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/list.\nClick on each cluster, and click on any Node pools that are not provisioned by default.\nOn the Node pool Details page, under the Security heading on the Node pool details page, check that Sandbox with gVisor is set to 'Enabled'.\n\nThe default node pool cannot use GKE Sandbox.\nUsing Command Line:\nRun this command:\ngcloud container node-pools describe $NODE_POOL --cluster $CLUSTER_NAME --zone $COMPUTE_ZONE --format json | jq '.config.sandboxConfig'\n\nThe output of the above command will return the following if the Node pool is running a sandbox:\n{\n  \"sandboxType\":\"gvisor\"\n}\n\nIf there is no sandbox, the above command output will be null ({ }).\nThe default node pool cannot use GKE Sandbox.",
    "remediation_procedure": "Once a node pool is created, GKE Sandbox cannot be enabled, rather a new node pool is required. The default node pool (the first node pool in your cluster, created when the cluster is created) cannot use GKE Sandbox.\nUsing Google Cloud Console:\n\nGo to Kubernetes Engine by visiting: https://console.cloud.google.com/kubernetes/.\nSelect a cluster and click ADD NODE POOL.\nConfigure the Node pool with following settings:\n\nFor the node version, select v1.12.6-gke.8 or higher.\nFor the node image, select Container-Optimized OS with Containerd (cos_containerd) (default).\nUnder Security, select Enable sandbox with gVisor.\n\n\nConfigure other Node pool settings as required.\nClick SAVE.\n\nUsing Command Line:\nTo enable GKE Sandbox on an existing cluster, a new Node pool must be created, which can be done using:\n  gcloud container node-pools create <node_pool_name> --zone <compute-zone> --cluster <cluster_name> --image-type=cos_containerd --sandbox=\"type=gvisor\"",
    "default_value": "By default, GKE Sandbox is disabled."
  }
]